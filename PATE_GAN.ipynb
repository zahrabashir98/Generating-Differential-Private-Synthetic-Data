{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ualberta_Pate_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPj2cDMTvBPy",
        "outputId": "7919fad5-5507-41a9-b86c-d8fb898cfda8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "'''\n",
        "Code inspired from:\n",
        "https://github.com/BorealisAI/private-data-generation/tree/master/models, \n",
        "https://github.com/tensorflow/privacy/tree/master/research/pate_2017\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCocKoRW0BpA"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suIGVEq2vCwV"
      },
      "source": [
        "## Load Credit Card Data\n",
        "\n",
        "Don't run this section. This section is for loading another dataset that we tested. The results were not satisfying, so, we changed the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Fz4PtzzN_Q"
      },
      "source": [
        "from pandas import read_csv\n",
        "dataframe = read_csv('/content/drive/MyDrive/Privacy/creditcard.csv', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RamAQ0ym_KRZ"
      },
      "source": [
        "from collections import Counter\n",
        "# summarize the class distribution\n",
        "target = dataframe.values[:,-1]\n",
        "counter = Counter(target)\n",
        "for k,v in counter.items():\n",
        "\tper = v / len(target) * 100\n",
        "\tprint('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wXl9KBS_bKS"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_labels = y.reshape((284807, 1))\n",
        "COND_num_classes = 2 # Number of classes\n",
        "train_labels_vec = np.zeros((len(train_labels), COND_num_classes), dtype='float32')\n",
        "for i, label in enumerate(train_labels):\n",
        "    train_labels_vec[i, int(train_labels[i])] = 1.0\n",
        "\n",
        "train_data = X.astype('float32')\n",
        "print(train_data.shape,train_labels_vec.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6LklDmoHE6F"
      },
      "source": [
        "## Load Banknote  Authentication  Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm2cqhWwHEEt",
        "outputId": "9a2aec06-401f-4ddb-bdeb-837124a3f994"
      },
      "source": [
        "from pandas import read_csv\n",
        "from scipy.special import expit\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_dataset(full_path = '/content/drive/MyDrive/Privacy/real_data_train.csv'):\n",
        "  # load the dataset as a numpy array\n",
        "  with open(full_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "  # retrieve numpy array\n",
        "  data = data.values\n",
        "  # split into input and output elements\n",
        "  X, y = data[:, :-1], data[:, -1]\n",
        "  return X, y\n",
        "\n",
        "X, y = load_dataset()\n",
        "# preprocessing\n",
        "X = expit(X)\n",
        "X, y = shuffle(X, y)\n",
        "print(X.shape,y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 4) (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j-ic3OWQLZm"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gwxWSB-QKjL"
      },
      "source": [
        "# Copyright 2019 RBC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size, output_size, conditional=False):\n",
        "        super().__init__()\n",
        "        z = latent_size\n",
        "        d = output_size\n",
        "        if conditional:\n",
        "            z = z + 1\n",
        "        else:\n",
        "            d = d + 1\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(z, 2 * latent_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2 * latent_size, d))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, wasserstein=False):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_size + 1, int(input_size / 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(input_size / 2), 1))\n",
        "\n",
        "        if not wasserstein:\n",
        "            self.main.add_module(str(3), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "def pate(data, netTD, lap_scale):\n",
        "\n",
        "    results = torch.Tensor(len(netTD), data.size()[0]).type(torch.int64)\n",
        "    for i in range(len(netTD)):\n",
        "        # print(netTD[i])\n",
        "        output = netTD[i].forward(data)\n",
        "        pred = (output > 0.5).type(torch.Tensor).squeeze()\n",
        "        results[i] = pred\n",
        "\n",
        "    clean_votes = torch.sum(results, dim=0).unsqueeze(1).type(torch.cuda.DoubleTensor)\n",
        "    noise = torch.from_numpy(np.random.laplace(loc=0, scale=1/lap_scale, size=clean_votes.size())).cuda()\n",
        "    noisy_results = clean_votes + noise\n",
        "    noisy_labels = (noisy_results > len(netTD)/2).type(torch.cuda.DoubleTensor)\n",
        "\n",
        "    return noisy_labels, clean_votes\n",
        "\n",
        "\n",
        "def moments_acc(num_teachers, clean_votes, lap_scale, l_list):\n",
        "    q = (2 + lap_scale * torch.abs(2*clean_votes - num_teachers)\n",
        "            )/(4 * torch.exp(lap_scale * torch.abs(2*clean_votes - num_teachers)))\n",
        "\n",
        "    update = []\n",
        "    for l in l_list:\n",
        "        a = 2*lap_scale*lap_scale*l*(l + 1)\n",
        "        t_one = (1 - q) * torch.pow((1 - q) / (1 - math.exp(2*lap_scale) * q), l)\n",
        "        t_two = q * torch.exp(2*lap_scale * l)\n",
        "        t = t_one + t_two\n",
        "        update.append(torch.clamp(t, max=a).sum())\n",
        "\n",
        "    return torch.cuda.DoubleTensor(update)\n",
        "\n",
        "\n",
        "def mutual_information(labels_x: pd.Series, labels_y: pd.DataFrame):\n",
        "\n",
        "    if labels_y.shape[1] == 1:\n",
        "        labels_y = labels_y.iloc[:, 0]\n",
        "    else:\n",
        "        labels_y = labels_y.apply(lambda x: ' '.join(x.get_values()), axis=1)\n",
        "\n",
        "    return mutual_info_score(labels_x, labels_y)\n",
        "\n",
        "\n",
        "def normalize_given_distribution(frequencies):\n",
        "    distribution = np.array(frequencies, dtype=float)\n",
        "    distribution = distribution.clip(0)  # replace negative values with 0\n",
        "    summation = distribution.sum()\n",
        "    if summation > 0:\n",
        "        if np.isinf(summation):\n",
        "            return normalize_given_distribution(np.isinf(distribution))\n",
        "        else:\n",
        "            return distribution / summation\n",
        "    else:\n",
        "        return np.full_like(distribution, 1 / distribution.size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_npQM4A0JEC"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvhP4iYpMXmr"
      },
      "source": [
        "## PATE GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7dlC0ysP--S"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "class PATE_GAN:\n",
        "    def __init__(self, input_dim, z_dim, num_teachers, target_epsilon, target_delta, conditional=True):\n",
        "\n",
        "        self.generator = Generator(z_dim, input_dim, conditional).cuda().double()\n",
        "        self.student_disc = Discriminator(input_dim, wasserstein=False).cuda().double()\n",
        "        self.teacher_disc = [Discriminator(input_dim, wasserstein=False).cuda().double()\n",
        "                             for _ in range(num_teachers)]\n",
        "        self.generator.apply(weights_init)\n",
        "        self.student_disc.apply(weights_init)\n",
        "        self.z_dim = z_dim\n",
        "        self.num_teachers = num_teachers\n",
        "\n",
        "        for i in range(num_teachers):\n",
        "            self.teacher_disc[i].apply(weights_init)\n",
        "\n",
        "        self.target_epsilon = target_epsilon\n",
        "        self.target_delta = target_delta\n",
        "        self.conditional = conditional\n",
        "\n",
        "    def train(self, x_train, y_train, hyperparams):\n",
        "        batch_size = hyperparams.batch_size\n",
        "        num_teacher_iters = hyperparams.num_teacher_iters\n",
        "        num_student_iters = hyperparams.num_student_iters\n",
        "        num_moments = hyperparams.num_moments\n",
        "        lap_scale = hyperparams.lap_scale\n",
        "        class_ratios = None\n",
        "        real_label = 1\n",
        "        fake_label = 0\n",
        "\n",
        "        alpha = torch.cuda.DoubleTensor([0.0 for _ in range(num_moments)])\n",
        "        l_list = 1 + torch.cuda.DoubleTensor(range(num_moments))\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        optimizer_g = optim.Adam(self.generator.parameters(), lr=hyperparams.lr)\n",
        "        optimizer_sd = optim.Adam(self.student_disc.parameters(), lr=hyperparams.lr)\n",
        "        optimizer_td = [optim.Adam(self.teacher_disc[i].parameters(), lr=hyperparams.lr\n",
        "                                   ) for i in range(self.num_teachers)]\n",
        "\n",
        "        tensor_data = data_utils.TensorDataset(torch.cuda.DoubleTensor(x_train), torch.cuda.DoubleTensor(y_train))\n",
        "        train_loader = []\n",
        "        for teacher_id in range(self.num_teachers):\n",
        "            start_id = teacher_id * len(tensor_data) / self.num_teachers\n",
        "            end_id = (teacher_id + 1) * len(tensor_data) / self.num_teachers if teacher_id != (\n",
        "                    self.num_teachers - 1) else len(tensor_data)\n",
        "\n",
        "            train_loader.append(data_utils.DataLoader(torch.utils.data.Subset( \\\n",
        "                tensor_data, range(int(start_id), int(end_id))), batch_size=batch_size, shuffle=True))\n",
        "\n",
        "        steps = 0\n",
        "        epsilon = 0\n",
        "\n",
        "        # while epsilon < self.target_epsilon:\n",
        "        while steps <=5000:\n",
        "            for t_2 in range(num_teacher_iters):\n",
        "                for i in range(self.num_teachers):\n",
        "                    inputs, categories = None, None\n",
        "                    for b, data in enumerate(train_loader[i], 0):\n",
        "                        inputs, categories = data\n",
        "                        break\n",
        "\n",
        "                    # train teachers with real\n",
        "                    optimizer_td[i].zero_grad()\n",
        "                    label = torch.full((inputs.size()[0],), real_label).cuda()\n",
        "                    output = self.teacher_disc[i].forward(torch.cat([inputs, categories.unsqueeze(1).double()], dim=1))\n",
        "                    output = torch.squeeze(output)\n",
        "                    err_d_real = criterion(output, label.double())\n",
        "\n",
        "                    err_d_real.backward()\n",
        "\n",
        "                    # train teachers with fake\n",
        "                    z = torch.Tensor(batch_size, self.z_dim).uniform_(0, 1).cuda()\n",
        "                    label.fill_(fake_label)\n",
        "\n",
        "                    if self.conditional:\n",
        "                        category = torch.multinomial(class_ratios,  inputs.size()[0], replacement=True).unsqueeze(1).cuda().double()\n",
        "                        fake = self.generator(torch.cat([z.double(), category], dim=1))\n",
        "                        output = self.teacher_disc[i].forward(torch.cat([fake.detach(), category], dim=1))\n",
        "                    else:\n",
        "                        fake = self.generator(z.double())\n",
        "                        output = self.teacher_disc[i].forward(fake)\n",
        "\n",
        "                    output = torch.squeeze(output)\n",
        "\n",
        "                    err_d_fake = criterion(output, label.double())\n",
        "                    err_d_fake.backward()\n",
        "                    optimizer_td[i].step()\n",
        "\n",
        "            # train the student discriminator\n",
        "            for t_3 in range(num_student_iters):\n",
        "                z = torch.Tensor(batch_size, self.z_dim).uniform_(0, 1).cuda()\n",
        "\n",
        "                if self.conditional:\n",
        "                    category = torch.multinomial(class_ratios,  inputs.size()[0], replacement=True).unsqueeze(1).cuda().double()\n",
        "                    fake = self.generator(torch.cat([z.double(), category], dim=1))\n",
        "                    predictions, clean_votes = pate(torch.cat(\n",
        "                        [fake.detach(), category], dim=1), self.teacher_disc, lap_scale)\n",
        "                    outputs = self.student_disc.forward(torch.cat([fake.detach(), category], dim=1))\n",
        "                else:\n",
        "                    fake = self.generator(z.double())\n",
        "                    predictions, clean_votes = pate(fake.detach(), self.teacher_disc, lap_scale)\n",
        "                    outputs = self.student_disc.forward(fake.detach())\n",
        "\n",
        "                # update the moments\n",
        "                alpha = alpha + moments_acc(self.num_teachers, clean_votes, lap_scale, l_list)\n",
        "\n",
        "                # update student\n",
        "                # predictions = torch.unsqueeze(predictions,1)\n",
        "                err_sd = criterion(outputs, predictions)\n",
        "\n",
        "                optimizer_sd.zero_grad()\n",
        "                err_sd.backward()\n",
        "                optimizer_sd.step()\n",
        "\n",
        "            # train the generator\n",
        "            optimizer_g.zero_grad()\n",
        "            z = torch.Tensor(batch_size, self.z_dim).uniform_(0, 1).cuda()\n",
        "            label = torch.full((inputs.size()[0],), real_label).cuda()\n",
        "\n",
        "            if self.conditional:\n",
        "                category = torch.multinomial(class_ratios,  inputs.size()[0], replacement=True).unsqueeze(1).cuda().double()\n",
        "                fake = self.generator(torch.cat([z.double(), category], dim=1))\n",
        "                output = self.student_disc(torch.cat([fake, category.double()], dim=1))\n",
        "            else:\n",
        "                fake = self.generator(z.double())\n",
        "                output = self.student_disc.forward(fake)\n",
        "            \n",
        "            output = torch.squeeze(output)\n",
        "            err_g = criterion(output, label.double())\n",
        "            err_g.backward()\n",
        "            optimizer_g.step()\n",
        "            # if steps %1000 == 0:\n",
        "            #   torch.save(self.generator.state_dict(), \"/content/drive/MyDrive/Privacy/model_step%d.h5\"%steps)\n",
        "            #   torch.save(self.generator, \"/content/drive/MyDrive/Privacy/entire_model_step%d.h5\"%steps)\n",
        "\n",
        "            # Calculate the privacy cost\n",
        "            epsilon = min((alpha - math.log(self.target_delta)) / l_list)\n",
        "            \n",
        "            if steps % 100 == 0:\n",
        "                print(\"Step : \", steps, \"Student Loss: \", err_sd.item(), \"Generator Loss: \", err_g.item(), \"Epsilon : \",\n",
        "                      epsilon.item())\n",
        "            steps += 1\n",
        "\n",
        "    def generate(self, num_rows, class_ratios, batch_size=1000):\n",
        "        steps = num_rows // batch_size\n",
        "        synthetic_data = []\n",
        "        if self.conditional:\n",
        "            class_ratios = torch.from_numpy(class_ratios)\n",
        "        for step in range(steps):\n",
        "            noise = torch.randn(batch_size, self.z_dim).cuda()\n",
        "            if self.conditional:\n",
        "                cat = torch.multinomial(class_ratios, batch_size, replacement=True).unsqueeze(1).cuda().double()\n",
        "                synthetic = self.generator(torch.cat([noise.double(), cat], dim=1))\n",
        "                synthetic = torch.cat([synthetic, cat], dim=1)\n",
        "\n",
        "            else:\n",
        "                synthetic = self.generator(noise.double())\n",
        "\n",
        "            synthetic_data.append(synthetic.cpu().data.numpy())\n",
        "\n",
        "        if steps * batch_size < num_rows:\n",
        "            noise = torch.randn(num_rows - steps * batch_size, self.z_dim).cuda()\n",
        "\n",
        "            if self.conditional:\n",
        "                cat = torch.multinomial(class_ratios, num_rows - steps * batch_size, replacement=True).unsqueeze(\n",
        "                    1).cuda().double()\n",
        "                synthetic = self.generator(torch.cat([noise.double(), cat], dim=1))\n",
        "                synthetic = torch.cat([synthetic, cat], dim=1)\n",
        "            else:\n",
        "                synthetic = self.generator(noise.double())\n",
        "            synthetic_data.append(synthetic.cpu().data.numpy())\n",
        "\n",
        "        return np.concatenate(synthetic_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFuRDV3fT4Tj"
      },
      "source": [
        "# Evaulate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAwmgNjdTfiu"
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.special import expit\n",
        "\n",
        "class_ratios = None\n",
        "target_variable = 4\n",
        "X_train = X\n",
        "y_train = y\n",
        "X_train_pos = []\n",
        "y_train_pos = []\n",
        "X_train_neg = []\n",
        "y_train_neg = []\n",
        "positive = True\n",
        "\n",
        "if positive:\n",
        "  #load positive data\n",
        "  for i in range(len(y_train)): \n",
        "      if y_train[i] == 1:\n",
        "          y_train_pos.append(y_train[i])\n",
        "          X_train_pos.append(X_train[i])\n",
        "  X_train = X_train_pos\n",
        "  y_train = y_train_pos\n",
        "\n",
        "else:\n",
        "  # load negative data\n",
        "  for i in range(len(y_train)): \n",
        "      if y_train[i] == 0:\n",
        "          y_train_neg.append(y_train[i])\n",
        "          X_train_neg.append(X_train[i])\n",
        "  X_train = X_train_neg\n",
        "  y_train = y_train_neg\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "print(X_train.shape)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "print(\"Input dim:\", input_dim)\n",
        "z_dim = int(input_dim / 4 + 1) if input_dim % 4 == 0 else int(input_dim / 4)\n",
        "print(\"Z dim:\", z_dim)\n",
        "\n",
        "# TODO check True\n",
        "conditional = False\n",
        "Hyperparams = collections.namedtuple('Hyperarams','batch_size num_teacher_iters num_student_iters num_moments lap_scale class_ratios lr')\n",
        "Hyperparams.__new__.__defaults__ = (None, None, None, None, None, None, None)\n",
        "\n",
        "num_teachers = 10\n",
        "target_epsilon = 8\n",
        "batch_size = 16\n",
        "student_iters = 5\n",
        "teacher_iters = 5\n",
        "num_moments = 100\n",
        "lap_scale = 0.0001\n",
        "target_delta = 1e-5\n",
        "\n",
        "model = PATE_GAN(input_dim, z_dim, num_teachers, target_epsilon, target_delta, conditional)\n",
        "model.train(X_train, y_train, Hyperparams(batch_size=batch_size, num_teacher_iters=teacher_iters,\n",
        "                                          num_student_iters=student_iters, num_moments=num_moments,\n",
        "                                          lap_scale=lap_scale, class_ratios=class_ratios, lr=1e-4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88XngDApx4ZN"
      },
      "source": [
        "## Generate New Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXEi59MbJq8F",
        "outputId": "7f972873-451f-4ac1-819a-b41b6b8f3ef7"
      },
      "source": [
        "syn_data = model.generate(4000, class_ratios)\n",
        "X_syn, y_syn = syn_data[:, :-1], syn_data[:, -1]\n",
        "print(X_syn.shape)\n",
        "print(y_syn.shape)\n",
        "\n",
        "if positive:\n",
        "  output = open(\"/content/drive/MyDrive/Privacy/generated data with pate/pate_pos_x_low4000.csv\", 'wb')\n",
        "  pickle.dump(X_syn, output)\n",
        "  output = open(\"/content/drive/MyDrive/Privacy/generated data with pate/pate_pos_y_low4000.csv\", 'wb')\n",
        "  pickle.dump(y_train, output)\n",
        "  output.close()\n",
        "else:\n",
        "  output = open(\"/content/drive/MyDrive/Privacy/generated data with pate/pate_neg_x_low4000.csv\", 'wb')\n",
        "  pickle.dump(X_syn, output)\n",
        "  output = open(\"/content/drive/MyDrive/Privacy/generated data with pate/pate_neg_x_low4000.csv\", 'wb')\n",
        "  pickle.dump(y_train, output)\n",
        "  output.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1979, 30)\n",
            "(1979,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGYxtrAqNYm8"
      },
      "source": [
        "## Load Generated Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2VvIaQ2LKSz",
        "outputId": "7082a7af-627f-4175-eaba-adc1148e3b3f"
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Privacy/synthetic_data_x.csv', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "print(data)\n",
        "\n",
        "with open('/content/drive/MyDrive/Privacy/synthetic_data_y.csv', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "# print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.06871043 -0.11243304 -0.0139605  ... -0.10280559 -0.29044451\n",
            "   0.21550894]\n",
            " [-0.12334208 -1.51443969  1.53745991 ... -2.57993242 -3.428202\n",
            "   2.38634704]\n",
            " [-0.49547485 -0.43342706  0.82718787 ... -0.8907703  -1.69688236\n",
            "   1.16405087]\n",
            " ...\n",
            " [-0.38557645 -0.24448216  0.75410519 ... -0.93036286 -2.57071541\n",
            "   1.42400039]\n",
            " [-0.37192661 -0.89022344  1.40215878 ... -1.26005326 -2.50563993\n",
            "   0.43152854]\n",
            " [-0.17935156 -1.59148958  1.48924768 ... -2.73311052 -3.34220363\n",
            "   1.55835475]]\n",
            "[-1.13458813e-01 -3.45445870e-01 -4.83912181e-01  3.63954521e-02\n",
            " -2.88157157e-01 -5.32594697e-01 -1.18384900e-01  9.97940486e-02\n",
            " -2.39628383e-01 -2.16450680e+00  2.34119683e-01 -3.84500547e-01\n",
            " -1.38524935e+00  7.19098340e-02 -1.13392250e+00 -3.35737769e-01\n",
            " -1.19312353e+00 -1.43163906e+00  4.89505476e-01 -1.74986167e-02\n",
            " -2.08120607e-01  2.78912805e-01  2.25208036e-03 -9.06280842e-01\n",
            "  1.91910146e-01 -1.52777441e-03  1.34045680e-01 -3.06395072e+00\n",
            " -2.85559853e-01 -1.17482253e+00 -1.61296529e+00 -8.57733298e-01\n",
            " -2.00381060e-02 -2.80591173e-01 -1.45174859e+00 -3.17390728e-01\n",
            " -1.79202493e+00 -1.41140276e-02 -6.46716109e-01 -4.86781719e-01\n",
            " -9.61947969e-01 -7.90186381e-01 -1.25188441e+00 -2.84497434e-01\n",
            " -2.06749431e+00 -1.65661955e+00 -2.66616134e-01 -6.53376469e-01\n",
            "  1.11830941e-01  7.29719249e-02  5.34227526e-01 -1.00728675e+00\n",
            " -1.23803174e+00 -1.08604199e+00 -6.52600449e-01 -7.23965725e-01\n",
            " -4.10825151e-02 -6.50393819e-01 -6.65418594e-01 -7.62290517e-01\n",
            "  2.41460217e-02 -3.66476996e-01 -4.97466039e-01 -1.37004344e-01\n",
            "  1.39339963e-01 -3.52140193e+00 -1.38933197e+00  3.59929882e-01\n",
            "  1.30682180e-01 -3.44194516e+00  1.20100317e-01  3.24116097e-01\n",
            " -1.69792320e+00 -5.89469695e-01 -6.02483215e-01  1.14604624e-01\n",
            "  2.06109961e-01 -1.41223669e-01  3.93301582e-02 -1.34901587e-01\n",
            " -1.56727410e+00 -1.77268069e+00  3.55599993e-01  3.51051766e-01\n",
            " -1.08022998e-01 -6.35053614e-01 -1.02482488e+00 -7.91376811e-02\n",
            " -5.88457014e-01  3.71651530e-01 -5.00564883e-01 -5.93292393e-02\n",
            " -4.42539696e-01  3.24109831e-01 -5.96309709e-01 -8.06344925e-01\n",
            "  1.49096491e-01 -3.49131396e-01 -1.36520177e+00 -1.53898143e+00\n",
            " -1.54081345e+00 -1.43116297e+00  4.14463426e-02 -4.07167337e-01\n",
            " -2.05402069e+00 -5.97145331e-01 -1.66178489e+00 -6.82665072e-01\n",
            " -7.76135922e-02  2.67297318e-01 -1.22812777e+00 -1.33574110e+00\n",
            " -5.31252288e-01 -4.76675525e-01 -5.90232036e-01 -3.33767451e-01\n",
            " -1.35222935e-03  2.30462435e-02 -9.75461268e-01  1.06119294e-01\n",
            " -1.15715630e+00  9.37250171e-02 -2.04338448e-01 -7.89428836e-01\n",
            " -7.71820217e-01 -2.56528649e-01 -2.92440755e-01 -1.33043658e-01\n",
            " -9.30332494e-02 -1.45926637e+00 -2.15171912e-01 -4.26567402e-01\n",
            "  2.41448512e-02 -1.00391347e+00  1.77960566e-01 -3.07932626e-01\n",
            " -1.13040800e+00 -1.34215731e+00  4.30222472e-02 -7.01748680e-01\n",
            " -9.41907202e-01 -9.06213333e-01 -3.97936924e-01  1.55267056e-01\n",
            " -4.18617861e-01 -4.68391711e-01 -7.30508544e-01  2.18060294e-01\n",
            " -7.91718832e-01 -1.20382985e-01 -1.29771297e-01 -7.48866845e-01\n",
            " -1.20089215e+00  4.28477143e-01 -3.91904750e-02 -1.08590609e+00\n",
            " -3.97442482e-01 -1.73504790e-01 -1.27927492e-01 -6.18770504e-01\n",
            "  1.05771249e-02 -3.42303606e-01 -2.05542740e+00 -8.55340375e-01\n",
            " -3.04701022e-01 -2.90849004e-01  1.68386065e-02  1.05771249e-02\n",
            " -1.36416432e+00  2.06845230e-01 -1.83318648e-01  8.20225136e-02\n",
            " -1.15716355e+00  1.37088785e-01  5.01634655e-03 -5.11035957e-01\n",
            " -3.37581081e-02 -1.72980221e+00 -1.77892030e-01  4.51519059e-04\n",
            " -1.58416297e+00 -1.46788817e-01  1.28926568e-01 -1.64709200e-01\n",
            " -1.63284517e-01 -4.67941216e-01 -1.90527385e-01  1.37644047e-01\n",
            " -1.20852838e-01 -9.66571389e-01 -8.44834215e-01 -1.64432857e+00\n",
            "  1.69443600e-01 -1.13797717e-01 -4.89525560e-01  1.92320562e-01\n",
            " -1.29648056e+00 -1.15906304e-02 -1.08293589e-01 -7.09779605e-01\n",
            "  5.65535250e-01  9.59464442e-01 -8.96487115e-01 -3.76318396e-01\n",
            "  2.93750481e-01 -1.28913852e-01 -5.91916250e-01 -1.35324626e+00\n",
            " -4.80490296e-01  7.21560337e-03 -1.58226238e+00 -6.16307731e-02\n",
            "  9.15660568e-03 -5.70207802e-01 -1.32183148e+00 -1.11661379e+00\n",
            " -4.35457184e-01  1.49890067e-01 -8.98936288e-02  2.14067431e-02\n",
            " -4.44326077e-03 -3.56533260e-01  5.44736475e-02 -5.73825175e-02\n",
            " -1.24268028e+00 -5.64485039e-02  1.22908177e-01 -2.09506093e-02\n",
            " -3.95673909e-02  1.99799694e-01 -3.24021406e-02  6.97819734e-02\n",
            " -2.33742699e+00 -1.66215387e+00 -3.57469256e-01  4.46396224e-01\n",
            " -2.03813930e+00 -1.25326260e+00 -1.36532687e-01 -7.33242688e-01\n",
            " -1.58894335e-01 -2.21095159e+00 -3.88608362e-01 -1.39147364e-01\n",
            "  1.88952607e-01  1.27651431e-01 -1.85821171e-01 -2.63113879e+00\n",
            " -8.99077559e-01 -6.73571711e-01 -2.78386875e+00 -1.33140272e-02\n",
            " -1.20265211e+00  4.06463686e-02 -2.37970668e-01 -3.51688445e-02\n",
            "  4.26042134e-01 -3.24197129e-01 -1.34528748e-01 -2.75958811e-01\n",
            " -4.85027793e-01 -5.14198867e-02 -1.46392814e+00 -1.05302505e+00\n",
            "  1.45173527e-01 -1.26125878e+00 -2.12432736e-01 -9.55791816e-02\n",
            "  8.26562182e-02  1.58999758e-01 -2.32612053e+00  1.05771249e-02\n",
            " -1.00114971e-01 -1.02385091e+00 -6.58955991e-01 -3.00044167e-01\n",
            " -4.68557529e-01  4.13327505e-02 -2.12998364e-01 -7.40799406e-01\n",
            " -1.28103566e+00  2.41843801e-01  2.74680822e-01 -8.76455968e-01\n",
            " -1.89431988e+00 -1.22577795e+00 -9.93234025e-01  5.18736723e-01\n",
            " -1.24057069e-01 -3.03666306e-01 -2.60864420e-01 -1.85312462e-01\n",
            " -8.96121261e-01 -6.32830443e-01  1.45701934e-01 -9.75055381e-01\n",
            " -1.71738487e-01 -1.43453539e+00 -1.14433484e+00 -3.37314348e-02\n",
            "  2.00691653e-01  1.03912425e-01  2.98601593e-01 -3.70750248e-01\n",
            " -1.54393697e-01 -7.83784838e-01 -1.28344495e-01 -4.86533435e-01\n",
            " -1.56253809e+00  4.15006477e-01 -1.91759242e+00 -1.15660358e-01\n",
            " -1.72862566e+00 -6.27873041e-03 -2.90928878e-01 -6.82049574e-01\n",
            " -1.52212541e-01 -6.63764826e-01 -4.46319501e-01  1.36613211e-01\n",
            " -4.23738523e-01 -4.10894842e-01 -1.64207490e+00 -4.30156046e-02\n",
            " -1.75792241e+00  1.85846511e-01 -1.00658496e-01 -7.40379424e-01\n",
            " -1.69461975e+00  1.87615266e-01 -1.71578583e+00 -5.00608248e-02\n",
            " -1.15896225e+00  6.97441150e-03 -2.10699377e-01 -5.70285514e-01\n",
            " -3.07914911e-01 -1.72984974e+00 -1.06801512e+00 -6.84285065e-01\n",
            " -3.82043075e-01 -9.27859188e-01  1.69127725e-01 -1.63482443e+00\n",
            " -1.25972483e+00 -1.64239498e-01  3.09350916e-02 -4.77198482e-01\n",
            " -4.33957373e-02 -3.45927425e-01 -9.89878364e-01 -2.07466219e-01\n",
            "  9.56652560e-03 -7.82514030e-01  3.01147654e-01  3.31991056e-01\n",
            " -5.41619827e-01  6.59801526e-01  2.19156943e-01 -9.13536476e-02\n",
            " -3.56349316e-01 -8.78713485e-01 -9.93232983e-01 -4.60464194e-01\n",
            " -1.55248898e-02 -5.43032300e-01 -2.48306774e-01  2.08844158e-01\n",
            " -3.44127395e-01 -1.47881228e+00 -7.38751560e-01 -1.11624781e+00\n",
            "  1.78938512e-01 -3.68845969e-02  1.05771249e-02 -8.84123587e-02\n",
            " -2.12179113e+00 -1.06295678e+00 -1.60908467e+00 -2.29520357e+00\n",
            " -2.43146491e-02  1.05690622e-03 -5.68550020e-01 -6.05133819e-01\n",
            " -3.23856806e-01 -5.65604214e-01 -8.86072710e-01  7.98278265e-02\n",
            " -1.58067875e-01 -5.16260796e-01 -2.12346890e-01  7.96118876e-02\n",
            " -9.40846895e-01 -2.43286817e-02  8.49119402e-02 -1.01645568e-01\n",
            " -1.04615684e+00 -1.60460017e+00 -2.73805244e-01  8.62702033e-02\n",
            " -7.39112346e-01 -1.44025281e+00 -2.11153041e-01 -5.25513841e-01\n",
            " -2.98536529e-01 -2.23908143e+00 -1.44002179e+00 -1.39090077e+00\n",
            " -3.54474509e-01 -1.08748815e-02 -1.14598223e+00 -7.06756605e-01\n",
            " -1.07774467e+00  4.16647540e-01  3.02094038e-01  4.51015744e-01\n",
            " -7.75608353e-01  4.65073262e-02 -1.09814075e+00  6.49679157e-01\n",
            " -1.18789441e+00 -5.37986780e-02  4.24924213e-02 -1.11555512e-02\n",
            " -1.43235848e+00 -1.96016212e+00 -1.04758684e+00 -2.39327069e-02\n",
            " -6.58887922e-01 -3.08683464e-01  1.75348176e-01 -1.30904725e+00\n",
            " -6.51202207e-01  1.28879223e-01  1.11162937e-01 -1.22616392e+00\n",
            " -1.53837121e+00 -1.16533161e-01  7.08826420e-03 -1.48601288e+00\n",
            " -1.35286051e+00 -2.43834362e-01 -2.51139618e-01  7.96171625e-02\n",
            "  1.23687797e-01 -9.45078052e-01  3.42015923e-02 -4.89328846e-01\n",
            " -1.19894289e+00 -2.53951149e-02 -1.22435222e+00 -2.36826328e-02\n",
            "  4.08901168e-02 -1.50954629e-02  5.30479987e-02 -2.52786637e-02\n",
            " -8.96189799e-01 -1.71659302e-02 -2.53867656e-01 -1.41260000e+00\n",
            " -1.98412099e-01  4.18074930e-01 -2.09164792e-01 -1.70756152e+00\n",
            " -1.36832557e+00 -4.91472056e-01 -1.93465371e+00 -7.86707852e-01\n",
            " -4.40090608e-02  4.15923452e-01 -7.97578734e-01  1.18231568e-01\n",
            "  5.31011541e-01  7.82309206e-01 -4.00233687e-01 -1.94193939e+00\n",
            " -5.17796658e-01 -9.72171168e-01 -4.05707805e-01 -1.25222951e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}