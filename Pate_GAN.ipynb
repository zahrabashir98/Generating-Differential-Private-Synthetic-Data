{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ualberta_Pate_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPj2cDMTvBPy",
        "outputId": "f481e01a-aa09-44b9-b752-931d47e56749"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suIGVEq2vCwV"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "f_Fz4PtzzN_Q",
        "outputId": "78e70e43-23ce-42e6-c6c2-e01ed0f2addb"
      },
      "source": [
        "from pandas import read_csv\n",
        "dataframe = read_csv('/content/drive/MyDrive/Privacy/creditcard.csv', header=None)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>284802</th>\n",
              "      <td>172786.0</td>\n",
              "      <td>-11.881118</td>\n",
              "      <td>10.071785</td>\n",
              "      <td>-9.834783</td>\n",
              "      <td>-2.066656</td>\n",
              "      <td>-5.364473</td>\n",
              "      <td>-2.606837</td>\n",
              "      <td>-4.918215</td>\n",
              "      <td>7.305334</td>\n",
              "      <td>1.914428</td>\n",
              "      <td>4.356170</td>\n",
              "      <td>-1.593105</td>\n",
              "      <td>2.711941</td>\n",
              "      <td>-0.689256</td>\n",
              "      <td>4.626942</td>\n",
              "      <td>-0.924459</td>\n",
              "      <td>1.107641</td>\n",
              "      <td>1.991691</td>\n",
              "      <td>0.510632</td>\n",
              "      <td>-0.682920</td>\n",
              "      <td>1.475829</td>\n",
              "      <td>0.213454</td>\n",
              "      <td>0.111864</td>\n",
              "      <td>1.014480</td>\n",
              "      <td>-0.509348</td>\n",
              "      <td>1.436807</td>\n",
              "      <td>0.250034</td>\n",
              "      <td>0.943651</td>\n",
              "      <td>0.823731</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284803</th>\n",
              "      <td>172787.0</td>\n",
              "      <td>-0.732789</td>\n",
              "      <td>-0.055080</td>\n",
              "      <td>2.035030</td>\n",
              "      <td>-0.738589</td>\n",
              "      <td>0.868229</td>\n",
              "      <td>1.058415</td>\n",
              "      <td>0.024330</td>\n",
              "      <td>0.294869</td>\n",
              "      <td>0.584800</td>\n",
              "      <td>-0.975926</td>\n",
              "      <td>-0.150189</td>\n",
              "      <td>0.915802</td>\n",
              "      <td>1.214756</td>\n",
              "      <td>-0.675143</td>\n",
              "      <td>1.164931</td>\n",
              "      <td>-0.711757</td>\n",
              "      <td>-0.025693</td>\n",
              "      <td>-1.221179</td>\n",
              "      <td>-1.545556</td>\n",
              "      <td>0.059616</td>\n",
              "      <td>0.214205</td>\n",
              "      <td>0.924384</td>\n",
              "      <td>0.012463</td>\n",
              "      <td>-1.016226</td>\n",
              "      <td>-0.606624</td>\n",
              "      <td>-0.395255</td>\n",
              "      <td>0.068472</td>\n",
              "      <td>-0.053527</td>\n",
              "      <td>24.79</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284804</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>1.919565</td>\n",
              "      <td>-0.301254</td>\n",
              "      <td>-3.249640</td>\n",
              "      <td>-0.557828</td>\n",
              "      <td>2.630515</td>\n",
              "      <td>3.031260</td>\n",
              "      <td>-0.296827</td>\n",
              "      <td>0.708417</td>\n",
              "      <td>0.432454</td>\n",
              "      <td>-0.484782</td>\n",
              "      <td>0.411614</td>\n",
              "      <td>0.063119</td>\n",
              "      <td>-0.183699</td>\n",
              "      <td>-0.510602</td>\n",
              "      <td>1.329284</td>\n",
              "      <td>0.140716</td>\n",
              "      <td>0.313502</td>\n",
              "      <td>0.395652</td>\n",
              "      <td>-0.577252</td>\n",
              "      <td>0.001396</td>\n",
              "      <td>0.232045</td>\n",
              "      <td>0.578229</td>\n",
              "      <td>-0.037501</td>\n",
              "      <td>0.640134</td>\n",
              "      <td>0.265745</td>\n",
              "      <td>-0.087371</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.026561</td>\n",
              "      <td>67.88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284805</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>-0.240440</td>\n",
              "      <td>0.530483</td>\n",
              "      <td>0.702510</td>\n",
              "      <td>0.689799</td>\n",
              "      <td>-0.377961</td>\n",
              "      <td>0.623708</td>\n",
              "      <td>-0.686180</td>\n",
              "      <td>0.679145</td>\n",
              "      <td>0.392087</td>\n",
              "      <td>-0.399126</td>\n",
              "      <td>-1.933849</td>\n",
              "      <td>-0.962886</td>\n",
              "      <td>-1.042082</td>\n",
              "      <td>0.449624</td>\n",
              "      <td>1.962563</td>\n",
              "      <td>-0.608577</td>\n",
              "      <td>0.509928</td>\n",
              "      <td>1.113981</td>\n",
              "      <td>2.897849</td>\n",
              "      <td>0.127434</td>\n",
              "      <td>0.265245</td>\n",
              "      <td>0.800049</td>\n",
              "      <td>-0.163298</td>\n",
              "      <td>0.123205</td>\n",
              "      <td>-0.569159</td>\n",
              "      <td>0.546668</td>\n",
              "      <td>0.108821</td>\n",
              "      <td>0.104533</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284806</th>\n",
              "      <td>172792.0</td>\n",
              "      <td>-0.533413</td>\n",
              "      <td>-0.189733</td>\n",
              "      <td>0.703337</td>\n",
              "      <td>-0.506271</td>\n",
              "      <td>-0.012546</td>\n",
              "      <td>-0.649617</td>\n",
              "      <td>1.577006</td>\n",
              "      <td>-0.414650</td>\n",
              "      <td>0.486180</td>\n",
              "      <td>-0.915427</td>\n",
              "      <td>-1.040458</td>\n",
              "      <td>-0.031513</td>\n",
              "      <td>-0.188093</td>\n",
              "      <td>-0.084316</td>\n",
              "      <td>0.041333</td>\n",
              "      <td>-0.302620</td>\n",
              "      <td>-0.660377</td>\n",
              "      <td>0.167430</td>\n",
              "      <td>-0.256117</td>\n",
              "      <td>0.382948</td>\n",
              "      <td>0.261057</td>\n",
              "      <td>0.643078</td>\n",
              "      <td>0.376777</td>\n",
              "      <td>0.008797</td>\n",
              "      <td>-0.473649</td>\n",
              "      <td>-0.818267</td>\n",
              "      <td>-0.002415</td>\n",
              "      <td>0.013649</td>\n",
              "      <td>217.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0          1          2   ...        28      29  30\n",
              "284802  172786.0 -11.881118  10.071785  ...  0.823731    0.77   0\n",
              "284803  172787.0  -0.732789  -0.055080  ... -0.053527   24.79   0\n",
              "284804  172788.0   1.919565  -0.301254  ... -0.026561   67.88   0\n",
              "284805  172788.0  -0.240440   0.530483  ...  0.104533   10.00   0\n",
              "284806  172792.0  -0.533413  -0.189733  ...  0.013649  217.00   0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO4ch3K2HCPk"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "lZqePCSfGhPi",
        "outputId": "f427927f-48aa-45a8-d3f2-0340161e9508"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "x = dataframe.values #returns a numpy array\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "df = pd.DataFrame(x_scaled)\n",
        "df.tail()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>284802</th>\n",
              "      <td>0.999965</td>\n",
              "      <td>0.756448</td>\n",
              "      <td>0.873531</td>\n",
              "      <td>0.666991</td>\n",
              "      <td>0.160317</td>\n",
              "      <td>0.729603</td>\n",
              "      <td>0.236810</td>\n",
              "      <td>0.235393</td>\n",
              "      <td>0.863749</td>\n",
              "      <td>0.528729</td>\n",
              "      <td>0.598850</td>\n",
              "      <td>0.190550</td>\n",
              "      <td>0.806406</td>\n",
              "      <td>0.394978</td>\n",
              "      <td>0.801627</td>\n",
              "      <td>0.267218</td>\n",
              "      <td>0.484577</td>\n",
              "      <td>0.789000</td>\n",
              "      <td>0.688412</td>\n",
              "      <td>0.509985</td>\n",
              "      <td>0.595979</td>\n",
              "      <td>0.564920</td>\n",
              "      <td>0.515249</td>\n",
              "      <td>0.680500</td>\n",
              "      <td>0.313600</td>\n",
              "      <td>0.658558</td>\n",
              "      <td>0.466291</td>\n",
              "      <td>0.433929</td>\n",
              "      <td>0.329840</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284803</th>\n",
              "      <td>0.999971</td>\n",
              "      <td>0.945845</td>\n",
              "      <td>0.766677</td>\n",
              "      <td>0.872678</td>\n",
              "      <td>0.219189</td>\n",
              "      <td>0.771561</td>\n",
              "      <td>0.273661</td>\n",
              "      <td>0.265504</td>\n",
              "      <td>0.788548</td>\n",
              "      <td>0.482925</td>\n",
              "      <td>0.488530</td>\n",
              "      <td>0.276355</td>\n",
              "      <td>0.738709</td>\n",
              "      <td>0.542361</td>\n",
              "      <td>0.623352</td>\n",
              "      <td>0.423414</td>\n",
              "      <td>0.426717</td>\n",
              "      <td>0.730383</td>\n",
              "      <td>0.569303</td>\n",
              "      <td>0.442620</td>\n",
              "      <td>0.580900</td>\n",
              "      <td>0.564933</td>\n",
              "      <td>0.553153</td>\n",
              "      <td>0.665619</td>\n",
              "      <td>0.245298</td>\n",
              "      <td>0.543855</td>\n",
              "      <td>0.360884</td>\n",
              "      <td>0.417775</td>\n",
              "      <td>0.312038</td>\n",
              "      <td>0.000965</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284804</th>\n",
              "      <td>0.999977</td>\n",
              "      <td>0.990905</td>\n",
              "      <td>0.764080</td>\n",
              "      <td>0.781102</td>\n",
              "      <td>0.227202</td>\n",
              "      <td>0.783425</td>\n",
              "      <td>0.293496</td>\n",
              "      <td>0.263547</td>\n",
              "      <td>0.792985</td>\n",
              "      <td>0.477677</td>\n",
              "      <td>0.498692</td>\n",
              "      <td>0.309763</td>\n",
              "      <td>0.706572</td>\n",
              "      <td>0.434111</td>\n",
              "      <td>0.628885</td>\n",
              "      <td>0.435700</td>\n",
              "      <td>0.453827</td>\n",
              "      <td>0.740239</td>\n",
              "      <td>0.680504</td>\n",
              "      <td>0.518236</td>\n",
              "      <td>0.580280</td>\n",
              "      <td>0.565220</td>\n",
              "      <td>0.537005</td>\n",
              "      <td>0.664877</td>\n",
              "      <td>0.468492</td>\n",
              "      <td>0.592824</td>\n",
              "      <td>0.411177</td>\n",
              "      <td>0.416593</td>\n",
              "      <td>0.312585</td>\n",
              "      <td>0.002642</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284805</th>\n",
              "      <td>0.999977</td>\n",
              "      <td>0.954209</td>\n",
              "      <td>0.772856</td>\n",
              "      <td>0.849587</td>\n",
              "      <td>0.282508</td>\n",
              "      <td>0.763172</td>\n",
              "      <td>0.269291</td>\n",
              "      <td>0.261175</td>\n",
              "      <td>0.792671</td>\n",
              "      <td>0.476287</td>\n",
              "      <td>0.500464</td>\n",
              "      <td>0.170288</td>\n",
              "      <td>0.667901</td>\n",
              "      <td>0.367667</td>\n",
              "      <td>0.661171</td>\n",
              "      <td>0.483042</td>\n",
              "      <td>0.429998</td>\n",
              "      <td>0.745946</td>\n",
              "      <td>0.729908</td>\n",
              "      <td>0.789612</td>\n",
              "      <td>0.581622</td>\n",
              "      <td>0.565755</td>\n",
              "      <td>0.547353</td>\n",
              "      <td>0.663008</td>\n",
              "      <td>0.398836</td>\n",
              "      <td>0.545958</td>\n",
              "      <td>0.514746</td>\n",
              "      <td>0.418520</td>\n",
              "      <td>0.315245</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284806</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.949232</td>\n",
              "      <td>0.765256</td>\n",
              "      <td>0.849601</td>\n",
              "      <td>0.229488</td>\n",
              "      <td>0.765632</td>\n",
              "      <td>0.256488</td>\n",
              "      <td>0.274963</td>\n",
              "      <td>0.780938</td>\n",
              "      <td>0.479528</td>\n",
              "      <td>0.489782</td>\n",
              "      <td>0.223414</td>\n",
              "      <td>0.703005</td>\n",
              "      <td>0.433771</td>\n",
              "      <td>0.643218</td>\n",
              "      <td>0.339417</td>\n",
              "      <td>0.439728</td>\n",
              "      <td>0.711942</td>\n",
              "      <td>0.664807</td>\n",
              "      <td>0.543314</td>\n",
              "      <td>0.584343</td>\n",
              "      <td>0.565688</td>\n",
              "      <td>0.540031</td>\n",
              "      <td>0.671029</td>\n",
              "      <td>0.383420</td>\n",
              "      <td>0.551319</td>\n",
              "      <td>0.291786</td>\n",
              "      <td>0.416466</td>\n",
              "      <td>0.313401</td>\n",
              "      <td>0.008446</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2   ...        28        29   30\n",
              "284802  0.999965  0.756448  0.873531  ...  0.329840  0.000030  0.0\n",
              "284803  0.999971  0.945845  0.766677  ...  0.312038  0.000965  0.0\n",
              "284804  0.999977  0.990905  0.764080  ...  0.312585  0.002642  0.0\n",
              "284805  0.999977  0.954209  0.772856  ...  0.315245  0.000389  0.0\n",
              "284806  1.000000  0.949232  0.765256  ...  0.313401  0.008446  0.0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RamAQ0ym_KRZ",
        "outputId": "7e80118a-4a0b-48f7-f08f-e0654c647ac4"
      },
      "source": [
        "from collections import Counter\n",
        "# summarize the class distribution\n",
        "target = dataframe.values[:,-1]\n",
        "counter = Counter(target)\n",
        "for k,v in counter.items():\n",
        "\tper = v / len(target) * 100\n",
        "\tprint('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class=0, Count=284315, Percentage=99.827%\n",
            "Class=1, Count=492, Percentage=0.173%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wXl9KBS_bKS",
        "outputId": "f9d823c9-37f4-4815-c014-39acb7ebd699"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_labels = y.reshape((284807, 1))\n",
        "COND_num_classes = 2 # Number of classes\n",
        "train_labels_vec = np.zeros((len(train_labels), COND_num_classes), dtype='float32')\n",
        "for i, label in enumerate(train_labels):\n",
        "    train_labels_vec[i, int(train_labels[i])] = 1.0\n",
        "\n",
        "train_data = X.astype('float32')\n",
        "print(train_data.shape,train_labels_vec.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(284807, 30) (284807, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j-ic3OWQLZm"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gwxWSB-QKjL"
      },
      "source": [
        "# Copyright 2019 RBC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size, output_size, conditional=False):\n",
        "        super().__init__()\n",
        "        z = latent_size\n",
        "        d = output_size\n",
        "        if conditional:\n",
        "            z = z + 1\n",
        "        else:\n",
        "            d = d + 1\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(z, 2 * latent_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2 * latent_size, d))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, wasserstein=False):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(input_size + 1, int(input_size / 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(int(input_size / 2), 1))\n",
        "\n",
        "        if not wasserstein:\n",
        "            self.main.add_module(str(3), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "#helper: \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "def pate(data, netTD, lap_scale):\n",
        "    results = torch.Tensor(len(netTD), data.size()[0]).type(torch.int64)\n",
        "    for i in range(len(netTD)):\n",
        "        output = netTD[i].forward(data)\n",
        "        pred = (output > 0.5).type(torch.Tensor).squeeze()\n",
        "        results[i] = pred\n",
        "\n",
        "    clean_votes = torch.sum(results, dim=0).unsqueeze(1).type(torch.cuda.DoubleTensor)\n",
        "    noise = torch.from_numpy(np.random.laplace(loc=0, scale=1/lap_scale, size=clean_votes.size())).cuda()\n",
        "    noisy_results = clean_votes + noise\n",
        "    noisy_labels = (noisy_results > len(netTD)/2).type(torch.cuda.DoubleTensor)\n",
        "\n",
        "    return noisy_labels, clean_votes\n",
        "\n",
        "\n",
        "def moments_acc(num_teachers, clean_votes, lap_scale, l_list):\n",
        "    q = (2 + lap_scale * torch.abs(2*clean_votes - num_teachers)\n",
        "         )/(4 * torch.exp(lap_scale * torch.abs(2*clean_votes - num_teachers)))\n",
        "\n",
        "    update = []\n",
        "    for l in l_list:\n",
        "        a = 2*lap_scale*lap_scale*l*(l + 1)\n",
        "        t_one = (1 - q) * torch.pow((1 - q) / (1 - math.exp(2*lap_scale) * q), l)\n",
        "        t_two = q * torch.exp(2*lap_scale * l)\n",
        "        t = t_one + t_two\n",
        "        update.append(torch.clamp(t, max=a).sum())\n",
        "\n",
        "    return torch.cuda.DoubleTensor(update)\n",
        "\n",
        "\n",
        "def mutual_information(labels_x: pd.Series, labels_y: pd.DataFrame):\n",
        "\n",
        "    if labels_y.shape[1] == 1:\n",
        "        labels_y = labels_y.iloc[:, 0]\n",
        "    else:\n",
        "        labels_y = labels_y.apply(lambda x: ' '.join(x.get_values()), axis=1)\n",
        "\n",
        "    return mutual_info_score(labels_x, labels_y)\n",
        "\n",
        "\n",
        "def normalize_given_distribution(frequencies):\n",
        "    distribution = np.array(frequencies, dtype=float)\n",
        "    distribution = distribution.clip(0)  # replace negative values with 0\n",
        "    summation = distribution.sum()\n",
        "    if summation > 0:\n",
        "        if np.isinf(summation):\n",
        "            return normalize_given_distribution(np.isinf(distribution))\n",
        "        else:\n",
        "            return distribution / summation\n",
        "    else:\n",
        "        return np.full_like(distribution, 1 / distribution.size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvhP4iYpMXmr"
      },
      "source": [
        "# PATE GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7dlC0ysP--S"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "import math\n",
        "# from utils.architectures import Generator, Discriminator\n",
        "# from utils.helper import weights_init, pate, moments_acc\n",
        "\n",
        "\n",
        "class PATE_GAN:\n",
        "    def __init__(self, input_dim, z_dim, num_teachers, target_epsilon, target_delta, conditional=True):\n",
        "        self.generator = Generator(z_dim, input_dim, conditional).cuda().double()\n",
        "        self.student_disc = Discriminator(input_dim, wasserstein=False).cuda().double()\n",
        "        self.teacher_disc = [Discriminator(input_dim, wasserstein=False).cuda().double()\n",
        "                             for _ in range(num_teachers)]\n",
        "        self.generator.apply(weights_init)\n",
        "        self.student_disc.apply(weights_init)\n",
        "        self.z_dim = z_dim\n",
        "        self.num_teachers = num_teachers\n",
        "        for i in range(num_teachers):\n",
        "            self.teacher_disc[i].apply(weights_init)\n",
        "\n",
        "        self.target_epsilon = target_epsilon\n",
        "        self.target_delta = target_delta\n",
        "        self.conditional = conditional\n",
        "\n",
        "    def train(self, x_train, y_train, hyperparams):\n",
        "        batch_size = hyperparams.batch_size\n",
        "        num_teacher_iters = hyperparams.num_teacher_iters\n",
        "        num_student_iters = hyperparams.num_student_iters\n",
        "        num_moments = hyperparams.num_moments\n",
        "        lap_scale = hyperparams.lap_scale\n",
        "        class_ratios = None\n",
        "        if self.conditional:\n",
        "            class_ratios = torch.from_numpy(hyperparams.class_ratios)\n",
        "\n",
        "        real_label = 1\n",
        "        fake_label = 0\n",
        "\n",
        "        alpha = torch.cuda.DoubleTensor([0.0 for _ in range(num_moments)])\n",
        "        l_list = 1 + torch.cuda.DoubleTensor(range(num_moments))\n",
        "        print(\"ghable loss\")\n",
        "        criterion = nn.BCELoss()\n",
        "        print(\"bade loss\")\n",
        "        optimizer_g = optim.Adam(self.generator.parameters(), lr=hyperparams.lr)\n",
        "        optimizer_sd = optim.Adam(self.student_disc.parameters(), lr=hyperparams.lr)\n",
        "        optimizer_td = [optim.Adam(self.teacher_disc[i].parameters(), lr=hyperparams.lr\n",
        "                                   ) for i in range(self.num_teachers)]\n",
        "\n",
        "        tensor_data = data_utils.TensorDataset(torch.cuda.DoubleTensor(x_train), torch.cuda.DoubleTensor(y_train))\n",
        "        print(\"HERE\")\n",
        "        train_loader = []\n",
        "        for teacher_id in range(self.num_teachers):\n",
        "            start_id = teacher_id * len(tensor_data) / self.num_teachers\n",
        "            end_id = (teacher_id + 1) * len(tensor_data) / self.num_teachers if teacher_id != (\n",
        "                    self.num_teachers - 1) else len(tensor_data)\n",
        "\n",
        "            train_loader.append(data_utils.DataLoader(torch.utils.data.Subset( \\\n",
        "                tensor_data, range(int(start_id), int(end_id))), batch_size=batch_size, shuffle=True))\n",
        "\n",
        "        steps = 0\n",
        "        epsilon = 0\n",
        "\n",
        "        # while epsilon < self.target_epsilon:PATE_GAN\n",
        "        while steps <=10000:\n",
        "            # train the teacher discriminators\n",
        "            for t_2 in range(num_teacher_iters):\n",
        "                for i in range(self.num_teachers):\n",
        "                    inputs, categories = None, None\n",
        "                    for b, data in enumerate(train_loader[i], 0):\n",
        "                        inputs, categories = data\n",
        "                        break\n",
        "\n",
        "                    # train with real\n",
        "                    optimizer_td[i].zero_grad()\n",
        "                    label = torch.full((inputs.size()[0],), real_label).cuda()\n",
        "                    output = self.teacher_disc[i].forward(torch.cat([inputs, categories.unsqueeze(1).double()], dim=1))\n",
        "                    output = torch.squeeze(output)\n",
        "                    err_d_real = criterion(output, label.double())\n",
        "\n",
        "                    err_d_real.backward()\n",
        "\n",
        "                    # train with fake\n",
        "                    z = torch.Tensor(batch_size, self.z_dim).uniform_(0, 1).cuda()\n",
        "                    label.fill_(fake_label)\n",
        "\n",
        "                    if self.conditional:\n",
        "                        category = torch.multinomial(class_ratios,  inputs.size()[0], replacement=True).unsqueeze(1).cuda().double()\n",
        "                        fake = self.generator(torch.cat([z.double(), category], dim=1))\n",
        "                        output = self.teacher_disc[i].forward(torch.cat([fake.detach(), category], dim=1))\n",
        "                    else:\n",
        "                        fake = self.generator(z.double())\n",
        "                        output = self.teacher_disc[i].forward(fake)\n",
        "\n",
        "                    output = torch.squeeze(output)\n",
        "\n",
        "                    err_d_fake = criterion(output, label.double())\n",
        "                    err_d_fake.backward()\n",
        "                    optimizer_td[i].step()\n",
        "\n",
        "            # train the student discriminator\n",
        "            for t_3 in range(num_student_iters):\n",
        "                z = torch.Tensor(batch_size, self.z_dim).uniform_(0, 1).cuda()\n",
        "\n",
        "                if self.conditional:\n",
        "                    category = torch.multinomial(class_ratios,  inputs.size()[0], replacement=True).unsqueeze(1).cuda().double()\n",
        "                    fake = self.generator(torch.cat([z.double(), category], dim=1))\n",
        "                    predictions, clean_votes = pate(torch.cat(\n",
        "                        [fake.detach(), category], dim=1), self.teacher_disc, lap_scale)\n",
        "                    outputs = self.student_disc.forward(torch.cat([fake.detach(), category], dim=1))\n",
        "                else:\n",
        "                    fake = self.generator(z.double())\n",
        "                    predictions, clean_votes = pate(fake.detach(), self.teacher_disc, lap_scale)\n",
        "                    outputs = self.student_disc.forward(fake.detach())\n",
        "\n",
        "                # update the moments\n",
        "                alpha = alpha + moments_acc(self.num_teachers, clean_votes, lap_scale, l_list)\n",
        "\n",
        "                # update student\n",
        "                # predictions = torch.unsqueeze(predictions,1)\n",
        "                # print(predictions.shape)\n",
        "                # print(predictions.shape)\n",
        "                err_sd = criterion(outputs, predictions)\n",
        "\n",
        "                optimizer_sd.zero_grad()\n",
        "                err_sd.backward()\n",
        "                optimizer_sd.step()\n",
        "\n",
        "            # train the generator\n",
        "            optimizer_g.zero_grad()\n",
        "            z = torch.Tensor(batch_size, self.z_dim).uniform_(0, 1).cuda()\n",
        "            label = torch.full((inputs.size()[0],), real_label).cuda()\n",
        "\n",
        "            if self.conditional:\n",
        "                category = torch.multinomial(class_ratios,  inputs.size()[0], replacement=True).unsqueeze(1).cuda().double()\n",
        "                fake = self.generator(torch.cat([z.double(), category], dim=1))\n",
        "                output = self.student_disc(torch.cat([fake, category.double()], dim=1))\n",
        "            else:\n",
        "                fake = self.generator(z.double())\n",
        "                output = self.student_disc.forward(fake)\n",
        "            \n",
        "            output = torch.squeeze(output)\n",
        "            err_g = criterion(output, label.double())\n",
        "            err_g.backward()\n",
        "            optimizer_g.step()\n",
        "            if steps %1000 == 0:\n",
        "              torch.save(self.generator.state_dict(), \"/content/drive/MyDrive/Privacy/model_step%d.h5\"%steps)\n",
        "              torch.save(self.generator, \"/content/drive/MyDrive/Privacy/entire_model_step%d.h5\"%steps)\n",
        "\n",
        "            # Calculate the current privacy cost\n",
        "            epsilon = min((alpha - math.log(self.target_delta)) / l_list)\n",
        "            if steps % 100 == 0:\n",
        "                print(\"Step : \", steps, \"Loss SD : \", err_sd.item(), \"Loss G : \", err_g.item(), \"Epsilon : \",\n",
        "                      epsilon.item())\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "    def generate(self, num_rows, class_ratios, batch_size=1000):\n",
        "        steps = num_rows // batch_size\n",
        "        synthetic_data = []\n",
        "        if self.conditional:\n",
        "            class_ratios = torch.from_numpy(class_ratios)\n",
        "        for step in range(steps):\n",
        "            noise = torch.randn(batch_size, self.z_dim).cuda()\n",
        "            if self.conditional:\n",
        "                cat = torch.multinomial(class_ratios, batch_size, replacement=True).unsqueeze(1).cuda().double()\n",
        "                synthetic = self.generator(torch.cat([noise.double(), cat], dim=1))\n",
        "                synthetic = torch.cat([synthetic, cat], dim=1)\n",
        "\n",
        "            else:\n",
        "                synthetic = self.generator(noise.double())\n",
        "\n",
        "            synthetic_data.append(synthetic.cpu().data.numpy())\n",
        "\n",
        "        if steps * batch_size < num_rows:\n",
        "            noise = torch.randn(num_rows - steps * batch_size, self.z_dim).cuda()\n",
        "\n",
        "            if self.conditional:\n",
        "                cat = torch.multinomial(class_ratios, num_rows - steps * batch_size, replacement=True).unsqueeze(\n",
        "                    1).cuda().double()\n",
        "                synthetic = self.generator(torch.cat([noise.double(), cat], dim=1))\n",
        "                synthetic = torch.cat([synthetic, cat], dim=1)\n",
        "            else:\n",
        "                synthetic = self.generator(noise.double())\n",
        "            synthetic_data.append(synthetic.cpu().data.numpy())\n",
        "\n",
        "        return np.concatenate(synthetic_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFuRDV3fT4Tj"
      },
      "source": [
        "# Evaulate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAwmgNjdTfiu"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
        "from sklearn import preprocessing\n",
        "from scipy.special import expit\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    from models.IMLE import imle\n",
        "except ImportError as error:\n",
        "    pass\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--categorical', action='store_true', help='All attributes of the data are categorical with small domains')\n",
        "parser.add_argument('--target-variable', help='Required if data has a target class')\n",
        "parser.add_argument('--train-data-path', required=True)\n",
        "parser.add_argument('--test-data-path', required=True)\n",
        "parser.add_argument('--normalize-data', action='store_true', help='Apply sigmoid function to each value in the data')\n",
        "parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')\n",
        "parser.add_argument('--downstream-task', default=\"classification\", help='classification | regression')\n",
        "\n",
        "privacy_parser = argparse.ArgumentParser(add_help=False)\n",
        "\n",
        "privacy_parser.add_argument('--enable-privacy', action='store_true', help='Enable private data generation')\n",
        "privacy_parser.add_argument('--target-epsilon', type=float, default=8, help='Epsilon differential privacy parameter')\n",
        "privacy_parser.add_argument('--target-delta', type=float, default=1e-5, help='Delta differential privacy parameter')\n",
        "privacy_parser.add_argument('--save-synthetic', action='store_true', help='Save the synthetic data into csv')\n",
        "privacy_parser.add_argument('--output-data-path', help='Required if synthetic data needs to be saved')\n",
        "\n",
        "\n",
        "\n",
        "noisy_sgd_parser = argparse.ArgumentParser(add_help=False)\n",
        "\n",
        "noisy_sgd_parser.add_argument('--sigma', type=float,\n",
        "                              default=2, help='Gaussian noise variance multiplier. A larger sigma will make the model '\n",
        "                                              'train for longer epochs for the same privacy budget')\n",
        "noisy_sgd_parser.add_argument('--clip-coeff', type=float,\n",
        "                              default=0.1, help='The coefficient to clip the gradients before adding noise for private '\n",
        "                                                'SGD training')\n",
        "noisy_sgd_parser.add_argument('--micro-batch-size',\n",
        "                              type=int, default=8,\n",
        "                              help='Parameter to tradeoff speed vs efficiency. Gradients are averaged for a microbatch '\n",
        "                                   'and then clipped before adding noise')\n",
        "\n",
        "noisy_sgd_parser.add_argument('--num-epochs', type=int, default=500)\n",
        "noisy_sgd_parser.add_argument('--batch-size', type=int, default=64)\n",
        "\n",
        "subparsers = parser.add_subparsers(help=\"generative model type\", dest=\"model\")\n",
        "\n",
        "parser_pate_gan = subparsers.add_parser('pate-gan', parents=[privacy_parser])\n",
        "parser_pate_gan.add_argument('--lap-scale', type=float,\n",
        "                             default=0.0001, help='Inverse laplace noise scale multiplier. A larger lap_scale will '\n",
        "                                                  'reduce the noise that is added per iteration of training.')\n",
        "parser_pate_gan.add_argument('--batch-size', type=int, default=64)\n",
        "parser_pate_gan.add_argument('--num-teachers', type=int, default=10, help=\"Number of teacher disciminators in the pate-gan model\")\n",
        "parser_pate_gan.add_argument('--teacher-iters', type=int, default=5, help=\"Teacher iterations during training per generator iteration\")\n",
        "parser_pate_gan.add_argument('--student-iters', type=int, default=5, help=\"Student iterations during training per generator iteration\")\n",
        "parser_pate_gan.add_argument('--num-moments', type=int, default=100, help=\"Number of higher moments to use for epsilon calculation for pate-gan\")\n",
        "\n",
        "# Loading the data\n",
        "# train = pd.read_csv(opt.train_data_path)\n",
        "# test = pd.read_csv(opt.test_data_path)\n",
        "\n",
        "train = dataframe \n",
        "# data_columns = [col for col in train.columns if col != opt.target_variable]\n",
        "# if opt.categorical:\n",
        "#     combined = train.append(test)\n",
        "#     config = {}\n",
        "#     for col in combined.columns:\n",
        "#         col_count = len(combined[col].unique())\n",
        "#         config[col] = col_count\n",
        "\n",
        "class_ratios = None\n",
        "\n",
        "# if opt.downstream_task == \"classification\":\n",
        "#     class_ratios = train[opt.target_variable].sort_values().groupby(train[opt.target_variable]).size().values/train.shape[0]\n",
        "\n",
        "\n",
        "target_variable = 30\n",
        "X_train = train.drop([target_variable], axis=1).values\n",
        "y_train = train[target_variable].values\n",
        "\n",
        "X_train_pos = []\n",
        "y_train_pos = []\n",
        "\n",
        "# load positive data\n",
        "# for i in range(len(y_train)): \n",
        "#     if len(X_train_pos) == 480:\n",
        "#         break\n",
        "#     if y_train[i] == 1:\n",
        "#         y_train_pos.append(y_train[i])\n",
        "#         X_train_pos.append(X_train[i])\n",
        "\n",
        "# load negative data\n",
        "for i in range(len(y_train)): \n",
        "    if y_train[i] == 0:\n",
        "        y_train_pos.append(y_train[i])\n",
        "        X_train_pos.append(X_train[i])\n",
        "\n",
        "X_train = np.array(X_train_pos)\n",
        "y_train = np.array(y_train_pos)\n",
        "\n",
        "\n",
        "print(np.array(X_train_pos).shape)\n",
        "print(np.array(y_train_pos).shape)\n",
        "\n",
        "# X_test = np.nan_to_num(test.drop([opt.target_variable], axis=1).values)\n",
        "# y_test = np.nan_to_num(test[opt.target_variable].values)\n",
        "\n",
        "# if opt.normalize_data:\n",
        "X_train = expit(X_train)\n",
        "    # X_test = expit(X_test)\n",
        "print(X_train.shape)\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "z_dim = int(input_dim / 4 + 1) if input_dim % 4 == 0 else int(input_dim / 4)\n",
        "\n",
        "# TODO check True\n",
        "conditional = False\n",
        "\n",
        "\n",
        "Hyperparams = collections.namedtuple(\n",
        "    'Hyperarams',\n",
        "    'batch_size num_teacher_iters num_student_iters num_moments lap_scale class_ratios lr')\n",
        "Hyperparams.__new__.__defaults__ = (None, None, None, None, None, None, None)\n",
        "\n",
        "num_teachers = 10\n",
        "target_epsilon = 8\n",
        "batch_size = 32\n",
        "student_iters = 5\n",
        "teacher_iters = 5\n",
        "num_moments = 100\n",
        "lap_scale = 0.0001\n",
        "target_delta = 1e-5\n",
        "model = PATE_GAN(input_dim, z_dim, num_teachers, target_epsilon, target_delta, conditional)\n",
        "model.train(X_train, y_train, Hyperparams(batch_size=batch_size, num_teacher_iters=teacher_iters,\n",
        "                                          num_student_iters=student_iters, num_moments=num_moments,\n",
        "                                          lap_scale=lap_scale, class_ratios=class_ratios, lr=1e-4))\n",
        "\n",
        "\n",
        "syn_data = model( X_train.shape[0], class_ratios)\n",
        "X_syn, y_syn = syn_data[:, :-1], syn_data[:, -1]\n",
        "print(X_syn.shape)\n",
        "print(y_syn.shape)\n",
        "\n",
        "output = open(\"/content/drive/MyDrive/Privacy/synthetic_data_x_negative.csv\", 'wb')\n",
        "pickle.dump(X_syn, output)\n",
        "output = open(\"/content/drive/MyDrive/Privacy/synthetic_data_y_negative.csv\", 'wb')\n",
        "pickle.dump(y_syn, output)\n",
        "output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGYxtrAqNYm8"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxAEg2guxBQT",
        "outputId": "bcf60979-b1af-445f-a3f7-140ec47f982e"
      },
      "source": [
        "# model = torch.load('/content/drive/MyDrive/Privacy/entire_model_step0.h5')\n",
        "# model.eval()\n",
        "# model = Generator(z_dim, input_dim, conditional)\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/Privacy/model_step1000.h5'))\n",
        "# model.eval()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(480, 30)\n",
            "(480,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2VvIaQ2LKSz",
        "outputId": "7082a7af-627f-4175-eaba-adc1148e3b3f"
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Privacy/synthetic_data_x.csv', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "print(data)\n",
        "\n",
        "with open('/content/drive/MyDrive/Privacy/synthetic_data_y.csv', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "# print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.06871043 -0.11243304 -0.0139605  ... -0.10280559 -0.29044451\n",
            "   0.21550894]\n",
            " [-0.12334208 -1.51443969  1.53745991 ... -2.57993242 -3.428202\n",
            "   2.38634704]\n",
            " [-0.49547485 -0.43342706  0.82718787 ... -0.8907703  -1.69688236\n",
            "   1.16405087]\n",
            " ...\n",
            " [-0.38557645 -0.24448216  0.75410519 ... -0.93036286 -2.57071541\n",
            "   1.42400039]\n",
            " [-0.37192661 -0.89022344  1.40215878 ... -1.26005326 -2.50563993\n",
            "   0.43152854]\n",
            " [-0.17935156 -1.59148958  1.48924768 ... -2.73311052 -3.34220363\n",
            "   1.55835475]]\n",
            "[-1.13458813e-01 -3.45445870e-01 -4.83912181e-01  3.63954521e-02\n",
            " -2.88157157e-01 -5.32594697e-01 -1.18384900e-01  9.97940486e-02\n",
            " -2.39628383e-01 -2.16450680e+00  2.34119683e-01 -3.84500547e-01\n",
            " -1.38524935e+00  7.19098340e-02 -1.13392250e+00 -3.35737769e-01\n",
            " -1.19312353e+00 -1.43163906e+00  4.89505476e-01 -1.74986167e-02\n",
            " -2.08120607e-01  2.78912805e-01  2.25208036e-03 -9.06280842e-01\n",
            "  1.91910146e-01 -1.52777441e-03  1.34045680e-01 -3.06395072e+00\n",
            " -2.85559853e-01 -1.17482253e+00 -1.61296529e+00 -8.57733298e-01\n",
            " -2.00381060e-02 -2.80591173e-01 -1.45174859e+00 -3.17390728e-01\n",
            " -1.79202493e+00 -1.41140276e-02 -6.46716109e-01 -4.86781719e-01\n",
            " -9.61947969e-01 -7.90186381e-01 -1.25188441e+00 -2.84497434e-01\n",
            " -2.06749431e+00 -1.65661955e+00 -2.66616134e-01 -6.53376469e-01\n",
            "  1.11830941e-01  7.29719249e-02  5.34227526e-01 -1.00728675e+00\n",
            " -1.23803174e+00 -1.08604199e+00 -6.52600449e-01 -7.23965725e-01\n",
            " -4.10825151e-02 -6.50393819e-01 -6.65418594e-01 -7.62290517e-01\n",
            "  2.41460217e-02 -3.66476996e-01 -4.97466039e-01 -1.37004344e-01\n",
            "  1.39339963e-01 -3.52140193e+00 -1.38933197e+00  3.59929882e-01\n",
            "  1.30682180e-01 -3.44194516e+00  1.20100317e-01  3.24116097e-01\n",
            " -1.69792320e+00 -5.89469695e-01 -6.02483215e-01  1.14604624e-01\n",
            "  2.06109961e-01 -1.41223669e-01  3.93301582e-02 -1.34901587e-01\n",
            " -1.56727410e+00 -1.77268069e+00  3.55599993e-01  3.51051766e-01\n",
            " -1.08022998e-01 -6.35053614e-01 -1.02482488e+00 -7.91376811e-02\n",
            " -5.88457014e-01  3.71651530e-01 -5.00564883e-01 -5.93292393e-02\n",
            " -4.42539696e-01  3.24109831e-01 -5.96309709e-01 -8.06344925e-01\n",
            "  1.49096491e-01 -3.49131396e-01 -1.36520177e+00 -1.53898143e+00\n",
            " -1.54081345e+00 -1.43116297e+00  4.14463426e-02 -4.07167337e-01\n",
            " -2.05402069e+00 -5.97145331e-01 -1.66178489e+00 -6.82665072e-01\n",
            " -7.76135922e-02  2.67297318e-01 -1.22812777e+00 -1.33574110e+00\n",
            " -5.31252288e-01 -4.76675525e-01 -5.90232036e-01 -3.33767451e-01\n",
            " -1.35222935e-03  2.30462435e-02 -9.75461268e-01  1.06119294e-01\n",
            " -1.15715630e+00  9.37250171e-02 -2.04338448e-01 -7.89428836e-01\n",
            " -7.71820217e-01 -2.56528649e-01 -2.92440755e-01 -1.33043658e-01\n",
            " -9.30332494e-02 -1.45926637e+00 -2.15171912e-01 -4.26567402e-01\n",
            "  2.41448512e-02 -1.00391347e+00  1.77960566e-01 -3.07932626e-01\n",
            " -1.13040800e+00 -1.34215731e+00  4.30222472e-02 -7.01748680e-01\n",
            " -9.41907202e-01 -9.06213333e-01 -3.97936924e-01  1.55267056e-01\n",
            " -4.18617861e-01 -4.68391711e-01 -7.30508544e-01  2.18060294e-01\n",
            " -7.91718832e-01 -1.20382985e-01 -1.29771297e-01 -7.48866845e-01\n",
            " -1.20089215e+00  4.28477143e-01 -3.91904750e-02 -1.08590609e+00\n",
            " -3.97442482e-01 -1.73504790e-01 -1.27927492e-01 -6.18770504e-01\n",
            "  1.05771249e-02 -3.42303606e-01 -2.05542740e+00 -8.55340375e-01\n",
            " -3.04701022e-01 -2.90849004e-01  1.68386065e-02  1.05771249e-02\n",
            " -1.36416432e+00  2.06845230e-01 -1.83318648e-01  8.20225136e-02\n",
            " -1.15716355e+00  1.37088785e-01  5.01634655e-03 -5.11035957e-01\n",
            " -3.37581081e-02 -1.72980221e+00 -1.77892030e-01  4.51519059e-04\n",
            " -1.58416297e+00 -1.46788817e-01  1.28926568e-01 -1.64709200e-01\n",
            " -1.63284517e-01 -4.67941216e-01 -1.90527385e-01  1.37644047e-01\n",
            " -1.20852838e-01 -9.66571389e-01 -8.44834215e-01 -1.64432857e+00\n",
            "  1.69443600e-01 -1.13797717e-01 -4.89525560e-01  1.92320562e-01\n",
            " -1.29648056e+00 -1.15906304e-02 -1.08293589e-01 -7.09779605e-01\n",
            "  5.65535250e-01  9.59464442e-01 -8.96487115e-01 -3.76318396e-01\n",
            "  2.93750481e-01 -1.28913852e-01 -5.91916250e-01 -1.35324626e+00\n",
            " -4.80490296e-01  7.21560337e-03 -1.58226238e+00 -6.16307731e-02\n",
            "  9.15660568e-03 -5.70207802e-01 -1.32183148e+00 -1.11661379e+00\n",
            " -4.35457184e-01  1.49890067e-01 -8.98936288e-02  2.14067431e-02\n",
            " -4.44326077e-03 -3.56533260e-01  5.44736475e-02 -5.73825175e-02\n",
            " -1.24268028e+00 -5.64485039e-02  1.22908177e-01 -2.09506093e-02\n",
            " -3.95673909e-02  1.99799694e-01 -3.24021406e-02  6.97819734e-02\n",
            " -2.33742699e+00 -1.66215387e+00 -3.57469256e-01  4.46396224e-01\n",
            " -2.03813930e+00 -1.25326260e+00 -1.36532687e-01 -7.33242688e-01\n",
            " -1.58894335e-01 -2.21095159e+00 -3.88608362e-01 -1.39147364e-01\n",
            "  1.88952607e-01  1.27651431e-01 -1.85821171e-01 -2.63113879e+00\n",
            " -8.99077559e-01 -6.73571711e-01 -2.78386875e+00 -1.33140272e-02\n",
            " -1.20265211e+00  4.06463686e-02 -2.37970668e-01 -3.51688445e-02\n",
            "  4.26042134e-01 -3.24197129e-01 -1.34528748e-01 -2.75958811e-01\n",
            " -4.85027793e-01 -5.14198867e-02 -1.46392814e+00 -1.05302505e+00\n",
            "  1.45173527e-01 -1.26125878e+00 -2.12432736e-01 -9.55791816e-02\n",
            "  8.26562182e-02  1.58999758e-01 -2.32612053e+00  1.05771249e-02\n",
            " -1.00114971e-01 -1.02385091e+00 -6.58955991e-01 -3.00044167e-01\n",
            " -4.68557529e-01  4.13327505e-02 -2.12998364e-01 -7.40799406e-01\n",
            " -1.28103566e+00  2.41843801e-01  2.74680822e-01 -8.76455968e-01\n",
            " -1.89431988e+00 -1.22577795e+00 -9.93234025e-01  5.18736723e-01\n",
            " -1.24057069e-01 -3.03666306e-01 -2.60864420e-01 -1.85312462e-01\n",
            " -8.96121261e-01 -6.32830443e-01  1.45701934e-01 -9.75055381e-01\n",
            " -1.71738487e-01 -1.43453539e+00 -1.14433484e+00 -3.37314348e-02\n",
            "  2.00691653e-01  1.03912425e-01  2.98601593e-01 -3.70750248e-01\n",
            " -1.54393697e-01 -7.83784838e-01 -1.28344495e-01 -4.86533435e-01\n",
            " -1.56253809e+00  4.15006477e-01 -1.91759242e+00 -1.15660358e-01\n",
            " -1.72862566e+00 -6.27873041e-03 -2.90928878e-01 -6.82049574e-01\n",
            " -1.52212541e-01 -6.63764826e-01 -4.46319501e-01  1.36613211e-01\n",
            " -4.23738523e-01 -4.10894842e-01 -1.64207490e+00 -4.30156046e-02\n",
            " -1.75792241e+00  1.85846511e-01 -1.00658496e-01 -7.40379424e-01\n",
            " -1.69461975e+00  1.87615266e-01 -1.71578583e+00 -5.00608248e-02\n",
            " -1.15896225e+00  6.97441150e-03 -2.10699377e-01 -5.70285514e-01\n",
            " -3.07914911e-01 -1.72984974e+00 -1.06801512e+00 -6.84285065e-01\n",
            " -3.82043075e-01 -9.27859188e-01  1.69127725e-01 -1.63482443e+00\n",
            " -1.25972483e+00 -1.64239498e-01  3.09350916e-02 -4.77198482e-01\n",
            " -4.33957373e-02 -3.45927425e-01 -9.89878364e-01 -2.07466219e-01\n",
            "  9.56652560e-03 -7.82514030e-01  3.01147654e-01  3.31991056e-01\n",
            " -5.41619827e-01  6.59801526e-01  2.19156943e-01 -9.13536476e-02\n",
            " -3.56349316e-01 -8.78713485e-01 -9.93232983e-01 -4.60464194e-01\n",
            " -1.55248898e-02 -5.43032300e-01 -2.48306774e-01  2.08844158e-01\n",
            " -3.44127395e-01 -1.47881228e+00 -7.38751560e-01 -1.11624781e+00\n",
            "  1.78938512e-01 -3.68845969e-02  1.05771249e-02 -8.84123587e-02\n",
            " -2.12179113e+00 -1.06295678e+00 -1.60908467e+00 -2.29520357e+00\n",
            " -2.43146491e-02  1.05690622e-03 -5.68550020e-01 -6.05133819e-01\n",
            " -3.23856806e-01 -5.65604214e-01 -8.86072710e-01  7.98278265e-02\n",
            " -1.58067875e-01 -5.16260796e-01 -2.12346890e-01  7.96118876e-02\n",
            " -9.40846895e-01 -2.43286817e-02  8.49119402e-02 -1.01645568e-01\n",
            " -1.04615684e+00 -1.60460017e+00 -2.73805244e-01  8.62702033e-02\n",
            " -7.39112346e-01 -1.44025281e+00 -2.11153041e-01 -5.25513841e-01\n",
            " -2.98536529e-01 -2.23908143e+00 -1.44002179e+00 -1.39090077e+00\n",
            " -3.54474509e-01 -1.08748815e-02 -1.14598223e+00 -7.06756605e-01\n",
            " -1.07774467e+00  4.16647540e-01  3.02094038e-01  4.51015744e-01\n",
            " -7.75608353e-01  4.65073262e-02 -1.09814075e+00  6.49679157e-01\n",
            " -1.18789441e+00 -5.37986780e-02  4.24924213e-02 -1.11555512e-02\n",
            " -1.43235848e+00 -1.96016212e+00 -1.04758684e+00 -2.39327069e-02\n",
            " -6.58887922e-01 -3.08683464e-01  1.75348176e-01 -1.30904725e+00\n",
            " -6.51202207e-01  1.28879223e-01  1.11162937e-01 -1.22616392e+00\n",
            " -1.53837121e+00 -1.16533161e-01  7.08826420e-03 -1.48601288e+00\n",
            " -1.35286051e+00 -2.43834362e-01 -2.51139618e-01  7.96171625e-02\n",
            "  1.23687797e-01 -9.45078052e-01  3.42015923e-02 -4.89328846e-01\n",
            " -1.19894289e+00 -2.53951149e-02 -1.22435222e+00 -2.36826328e-02\n",
            "  4.08901168e-02 -1.50954629e-02  5.30479987e-02 -2.52786637e-02\n",
            " -8.96189799e-01 -1.71659302e-02 -2.53867656e-01 -1.41260000e+00\n",
            " -1.98412099e-01  4.18074930e-01 -2.09164792e-01 -1.70756152e+00\n",
            " -1.36832557e+00 -4.91472056e-01 -1.93465371e+00 -7.86707852e-01\n",
            " -4.40090608e-02  4.15923452e-01 -7.97578734e-01  1.18231568e-01\n",
            "  5.31011541e-01  7.82309206e-01 -4.00233687e-01 -1.94193939e+00\n",
            " -5.17796658e-01 -9.72171168e-01 -4.05707805e-01 -1.25222951e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}