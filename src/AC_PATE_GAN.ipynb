{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AC-Pate-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "suIGVEq2vCwV"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPj2cDMTvBPy",
        "outputId": "08735ef6-6cd8-458e-c80b-7ea0bbf582a0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'''\n",
        "Code inspired from: (They were just inspirations, up to our knowledge, AC_PATE_GAN is not implemented so far.)\n",
        "https://github.com/BorealisAI/private-data-generation/tree/master/models, \n",
        "https://github.com/tensorflow/privacy/tree/master/research/pate_2017,\n",
        "https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/acgan/acgan.py,\n",
        "https://github.com/greenelab/SPRINT_gan/blob/master/ac_gan.py\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd50ib5k2WFU"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suIGVEq2vCwV"
      },
      "source": [
        "## Load Credit Card Data\n",
        "\n",
        "Don't run this section. This section is for loading another dataset that we tested. The results were not satisfying, so, we changed the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "f_Fz4PtzzN_Q",
        "outputId": "785dcd85-bc9f-4342-852a-168e52131c73"
      },
      "source": [
        "from pandas import read_csv\n",
        "dataframe = read_csv('/content/drive/MyDrive/Privacy/creditcard.csv', header=None)\n",
        "dataframe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0         1         2         3   ...        27        28      29  30\n",
              "0  0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62   0\n",
              "1  0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69   0\n",
              "2  1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66   0\n",
              "3  1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50   0\n",
              "4  2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99   0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RamAQ0ym_KRZ",
        "outputId": "60eb7157-61c8-4360-d965-4c4a12ad3bcd"
      },
      "source": [
        "from collections import Counter\n",
        "# summarize the class distribution\n",
        "target = dataframe.values[:,-1]\n",
        "counter = Counter(target)\n",
        "for k,v in counter.items():\n",
        "\tper = v / len(target) * 100\n",
        "\tprint('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class=0, Count=284315, Percentage=99.827%\n",
            "Class=1, Count=492, Percentage=0.173%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "-wXl9KBS_bKS",
        "outputId": "ce50c1d1-12e2-4c69-edf9-b30b441f721b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_labels = y.reshape((284807, 1))\n",
        "COND_num_classes = 2 # Number of classes\n",
        "train_labels_vec = np.zeros((len(train_labels), COND_num_classes), dtype='float32')\n",
        "for i, label in enumerate(train_labels):\n",
        "    train_labels_vec[i, int(train_labels[i])] = 1.0\n",
        "\n",
        "train_data = X.astype('float32')\n",
        "print(train_data.shape,train_labels_vec.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f5b65ceae993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m284807\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mCOND_num_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m# Number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_labels_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOND_num_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YmwH3W5z2Ob"
      },
      "source": [
        "## Load Banknote  Authentication  Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7QBRJCJz4Ce"
      },
      "source": [
        "from pandas import read_csv\n",
        "from scipy.special import expit\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_dataset(full_path = '/content/drive/MyDrive/Privacy/real_data_train.csv'):\n",
        "  # load the dataset as a numpy array\n",
        "  with open(full_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "  # retrieve numpy array\n",
        "  data = data.values\n",
        "  # split into input and output elements\n",
        "  X, y = data[:, :-1], data[:, -1]\n",
        "  return X, y\n",
        "\n",
        "X, y = load_dataset()\n",
        "# preprocessing\n",
        "X = expit(X)\n",
        "X, y = shuffle(X, y)\n",
        "print(X.shape,y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j-ic3OWQLZm"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gwxWSB-QKjL"
      },
      "source": [
        "# Copyright 2019 RBC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
        "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
        "    def __init__(self, latent_size, input_dim):\n",
        "        \n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            \n",
        "            nn.Linear((1 +latent_size), 2*latent_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2*latent_size, input_dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, label):\n",
        "       \n",
        "        x = torch.cat([input, label], 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
        "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 20), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 10),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.dc = nn.Sequential(\n",
        "            nn.Linear(10, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.cl = nn.Sequential(\n",
        "            nn.Linear(10, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.fc1(input)\n",
        "        d = self.dc(x)\n",
        "        c = self.cl(x)\n",
        "        return d, c\n",
        "\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "def pate(data, netTD, lap_scale):\n",
        "    results = torch.Tensor(len(netTD), data.size()[0]).type(torch.int64)\n",
        "    for i in range(len(netTD)):\n",
        "        output, aux = netTD[i].forward(data)\n",
        "        pred = (output > 0.5).type(torch.Tensor).squeeze()\n",
        "        results[i] = pred\n",
        "\n",
        "    clean_votes = torch.sum(results, dim=0).unsqueeze(1).type(torch.cuda.DoubleTensor)\n",
        "    noise = torch.from_numpy(np.random.laplace(loc=0, scale=1/lap_scale, size=clean_votes.size())).cuda()\n",
        "    noisy_results = clean_votes + noise\n",
        "    noisy_labels = (noisy_results > len(netTD)/2).type(torch.cuda.DoubleTensor)\n",
        "\n",
        "    return noisy_labels, clean_votes\n",
        "\n",
        "\n",
        "def moments_acc(num_teachers, clean_votes, lap_scale, l_list):\n",
        "    q = (2 + lap_scale * torch.abs(2*clean_votes - num_teachers)\n",
        "         )/(4 * torch.exp(lap_scale * torch.abs(2*clean_votes - num_teachers)))\n",
        "\n",
        "    update = []\n",
        "    for l in l_list:\n",
        "        a = 2*lap_scale*lap_scale*l*(l + 1)\n",
        "        t_one = (1 - q) * torch.pow((1 - q) / (1 - math.exp(2*lap_scale) * q), l)\n",
        "        t_two = q * torch.exp(2*lap_scale * l)\n",
        "        t = t_one + t_two\n",
        "        update.append(torch.clamp(t, max=a).sum())\n",
        "\n",
        "    return torch.cuda.DoubleTensor(update)\n",
        "\n",
        "\n",
        "def mutual_information(labels_x: pd.Series, labels_y: pd.DataFrame):\n",
        "\n",
        "    if labels_y.shape[1] == 1:\n",
        "        labels_y = labels_y.iloc[:, 0]\n",
        "    else:\n",
        "        labels_y = labels_y.apply(lambda x: ' '.join(x.get_values()), axis=1)\n",
        "\n",
        "    return mutual_info_score(labels_x, labels_y)\n",
        "\n",
        "\n",
        "def normalize_given_distribution(frequencies):\n",
        "    distribution = np.array(frequencies, dtype=float)\n",
        "    distribution = distribution.clip(0)  # replace negative values with 0\n",
        "    summation = distribution.sum()\n",
        "    if summation > 0:\n",
        "        if np.isinf(summation):\n",
        "            return normalize_given_distribution(np.isinf(distribution))\n",
        "        else:\n",
        "            return distribution / summation\n",
        "    else:\n",
        "        return np.full_like(distribution, 1 / distribution.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwlisSBf5RyK"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOt-q3H95Oyq"
      },
      "source": [
        "## ACPATE GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7dlC0ysP--S"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "class PATE_ACGAN:\n",
        "    def __init__(self, input_dim, z_dim, num_teachers, target_epsilon, target_delta):\n",
        "        self.generator = Generator(z_dim, input_dim).cuda().double()\n",
        "        self.student_disc = Discriminator(input_dim).cuda().double()\n",
        "        self.teacher_disc = [Discriminator(input_dim).cuda().double()\n",
        "                             for _ in range(num_teachers)]\n",
        "        self.generator.apply(weights_init)\n",
        "        self.student_disc.apply(weights_init)\n",
        "        self.z_dim = z_dim\n",
        "        self.num_teachers = num_teachers\n",
        "        for i in range(num_teachers):\n",
        "            self.teacher_disc[i].apply(weights_init)\n",
        "\n",
        "        self.target_epsilon = target_epsilon\n",
        "        self.target_delta = target_delta\n",
        "        self.conditional = False\n",
        "\n",
        "\n",
        "    def train(self, x_train, y_train, hyperparams):\n",
        "        batch_size = hyperparams.batch_size\n",
        "        num_teacher_iters = hyperparams.num_teacher_iters\n",
        "        num_student_iters = hyperparams.num_student_iters\n",
        "        num_moments = hyperparams.num_moments\n",
        "        lap_scale = hyperparams.lap_scale\n",
        "        class_ratios = torch.from_numpy(hyperparams.class_ratios)\n",
        "\n",
        "        real_label = 1\n",
        "        fake_label = 0\n",
        "\n",
        "        alpha = torch.cuda.DoubleTensor([0.0 for _ in range(num_moments)])\n",
        "        l_list = 1 + torch.cuda.DoubleTensor(range(num_moments))\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer_g = optim.Adam(self.generator.parameters(), lr=hyperparams.lr)\n",
        "        optimizer_sd = optim.Adam(self.student_disc.parameters(), lr=hyperparams.lr)\n",
        "        optimizer_td = [optim.Adam(self.teacher_disc[i].parameters(), lr=hyperparams.lr\n",
        "                                   ) for i in range(self.num_teachers)]\n",
        "\n",
        "        tensor_data = data_utils.TensorDataset(torch.cuda.DoubleTensor(x_train), torch.cuda.DoubleTensor(y_train))\n",
        "        train_loader = []\n",
        "        for teacher_id in range(self.num_teachers):\n",
        "            start_id = teacher_id * len(tensor_data) / self.num_teachers\n",
        "            end_id = (teacher_id + 1) * len(tensor_data) / self.num_teachers if teacher_id != (\n",
        "                    self.num_teachers - 1) else len(tensor_data)\n",
        "\n",
        "            train_loader.append(data_utils.DataLoader(torch.utils.data.Subset( \\\n",
        "                tensor_data, range(int(start_id), int(end_id))), batch_size=batch_size, shuffle=True))\n",
        "\n",
        "        steps = 0\n",
        "        epsilon = 0\n",
        "\n",
        "        # while epsilon < self.target_epsilon:\n",
        "        while steps <= 1000:\n",
        "\n",
        "            # train the teacher discriminators\n",
        "            for t_2 in range(num_teacher_iters):\n",
        "                for i in range(self.num_teachers):\n",
        "                    inputs, categories = None, None\n",
        "                    for b, data in enumerate(train_loader[i], 0):\n",
        "                        inputs, categories = data\n",
        "                        break\n",
        "\n",
        "                    # train teachers with real\n",
        "                    optimizer_td[i].zero_grad()\n",
        "                    label = torch.full((inputs.size()[0],), real_label).cuda()\n",
        "                    output, aux = self.teacher_disc[i].forward(inputs)\n",
        "                    output = torch.squeeze(output)\n",
        "                    \n",
        "                    err_d_real = criterion(output, label.double())\n",
        "                    # adding losses, and then doing the backpropagation\n",
        "                    err_d_real += criterion(aux, categories.unsqueeze(1).double())\n",
        "                    err_d_real.backward()\n",
        "\n",
        "                    # train teachers with fake\n",
        "                    z = torch.Tensor(batch_size, self.z_dim).uniform_(0, 1).cuda()\n",
        "                    label.fill_(fake_label)\n",
        "\n",
        "                    category = torch.multinomial(class_ratios,  inputs.size()[0], replacement=True).unsqueeze(1).cuda().double()\n",
        "                    fake = self.generator(z.double(), category)\n",
        "                    output, aux = self.teacher_disc[i].forward(fake)\n",
        "\n",
        "                    output = torch.squeeze(output)\n",
        "\n",
        "                    err_d_fake = criterion(output, label.double())\n",
        "                    # adding losses, and then doing the backpropagation\n",
        "                    err_d_fake += criterion(aux, categories.unsqueeze(1).double())\n",
        "                    err_d_fake.backward()\n",
        "\n",
        "                    optimizer_td[i].step()\n",
        "\n",
        "            # train the student discriminator\n",
        "            for t_3 in range(num_student_iters):\n",
        "                z = torch.Tensor(batch_size, self.z_dim).uniform_(0, 1).cuda()\n",
        "\n",
        "                category = torch.multinomial(class_ratios,  inputs.size()[0], replacement=True).unsqueeze(1).cuda().double()\n",
        "                fake = self.generator(z.double(), category)\n",
        "               \n",
        "                predictions, clean_votes = pate(fake.detach(), self.teacher_disc, lap_scale)\n",
        "                outputs, aux = self.student_disc.forward(fake.detach())\n",
        "\n",
        "                # update the moments\n",
        "                alpha = alpha + moments_acc(self.num_teachers, clean_votes, lap_scale, l_list)\n",
        "\n",
        "                # update student\n",
        "                err_sd = criterion(outputs, predictions)\n",
        "                optimizer_sd.zero_grad()\n",
        "\n",
        "                # adding losses, and then doing the backpropagation\n",
        "                err_sd += criterion(aux, categories.unsqueeze(1).double())\n",
        "                err_sd.backward()\n",
        "                optimizer_sd.step()\n",
        "            # train the generator\n",
        "            optimizer_g.zero_grad()\n",
        "            z = torch.Tensor(batch_size, self.z_dim).uniform_(0, 1).cuda()\n",
        "            label = torch.full((inputs.size()[0],), real_label).cuda()\n",
        "\n",
        "            category = torch.multinomial(class_ratios,  inputs.size()[0], replacement=True).unsqueeze(1).cuda().double()\n",
        "            fake = self.generator(z.double(), category)\n",
        "            output, aux = self.student_disc.forward(fake)\n",
        "            \n",
        "            output = torch.squeeze(output)\n",
        "            err_g = criterion(output, label.double())\n",
        "\n",
        "            # adding losses, and then doing the backpropagation\n",
        "            err_g += criterion(aux, categories.unsqueeze(1).double())\n",
        "            err_g.backward()\n",
        "            optimizer_g.step()\n",
        "            # if steps %1000 == 0:\n",
        "            #   torch.save(self.generator.state_dict(), \"/content/drive/MyDrive/Privacy/model_step%d.h5\"%steps)\n",
        "            #   torch.save(self.generator, \"/content/drive/MyDrive/Privacy/entire_model_step%d.h5\"%steps)\n",
        "\n",
        "            # Calculate the privacy cost\n",
        "            epsilon = min((alpha - math.log(self.target_delta)) / l_list)\n",
        "            if steps % 100 == 0:\n",
        "                print(\"Step : \", steps, \"Loss SD : \", err_sd.item(), \"Loss G : \", err_g.item(), \"Epsilon : \",\n",
        "                      epsilon.item())\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "\n",
        "    def generate(self, num_rows, class_ratios, batch_size=1):\n",
        "        steps = num_rows // batch_size\n",
        "        print(steps,num_rows)\n",
        "        synthetic_data = []\n",
        "        synthetic_label = []\n",
        "        class_ratios = torch.from_numpy(class_ratios)\n",
        "        for step in range(steps):\n",
        "            noise = torch.randn(batch_size, self.z_dim).cuda()\n",
        "\n",
        "            category = torch.multinomial(class_ratios, batch_size, replacement=True).unsqueeze(1).cuda().double()\n",
        "            synthetic = self.generator(noise.double(), category)\n",
        "\n",
        "            synthetic_data.append(synthetic.cpu().data.numpy())\n",
        "            synthetic_label.append(category.cpu().data.numpy())\n",
        "            \n",
        "        print(len(synthetic_data),len(synthetic_label))\n",
        "        return synthetic_data, synthetic_label"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFuRDV3fT4Tj"
      },
      "source": [
        "# Evaulate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAwmgNjdTfiu",
        "outputId": "53aa9154-1f6e-4c1b-839f-a7001e43f5df"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
        "from sklearn import preprocessing\n",
        "from scipy.special import expit\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import os\n",
        "\n",
        "\n",
        "# Loading the data\n",
        "class_ratios = None\n",
        "target_variable = 4\n",
        "X_train = np.array(X)\n",
        "y_train = np.array(y)\n",
        "\n",
        "X_train = expit(X_train)\n",
        "print(X_train.shape)\n",
        "input_dim = X_train.shape[1]\n",
        "z_dim = int(input_dim / 4 + 1) if input_dim % 4 == 0 else int(input_dim / 4)\n",
        "\n",
        "\n",
        "\n",
        "Hyperparams = collections.namedtuple('Hyperarams', 'batch_size num_teacher_iters num_student_iters num_moments lap_scale class_ratios lr')\n",
        "Hyperparams.__new__.__defaults__ = (None, None, None, None, None, None, None)\n",
        "\n",
        "num_teachers = 10\n",
        "target_epsilon = 8\n",
        "batch_size = 16\n",
        "student_iters = 5\n",
        "teacher_iters = 5\n",
        "num_moments = 100\n",
        "lap_scale = 0.0001\n",
        "target_delta = 1e-5\n",
        "model = PATE_ACGAN(input_dim, z_dim, num_teachers, target_epsilon, target_delta)\n",
        "model.train(X_train, y_train, Hyperparams(batch_size=batch_size, num_teacher_iters=teacher_iters,\n",
        "                                          num_student_iters=student_iters, num_moments=num_moments,\n",
        "                                          lap_scale=lap_scale, class_ratios=class_ratios, lr=1e-4))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1372, 4)\n",
            "ghable loss\n",
            "bade loss\n",
            "HERE\n",
            "Step :  0 Loss SD :  1.400318656377046 Loss G :  1.442842657272163 Epsilon :  0.11529085464970228\n",
            "Step :  100 Loss SD :  1.3764717922792942 Loss G :  1.407643440966949 Epsilon :  0.13145085464970213\n",
            "Step :  200 Loss SD :  1.3606291744990116 Loss G :  1.3775815986391144 Epsilon :  0.1476108546497025\n",
            "Step :  300 Loss SD :  1.3723837476480256 Loss G :  1.3808966421692728 Epsilon :  0.16377085464970192\n",
            "Step :  400 Loss SD :  1.34409529624027 Loss G :  1.3513895162905265 Epsilon :  0.17993085464970032\n",
            "Step :  500 Loss SD :  1.423035929873897 Loss G :  1.4377663173075572 Epsilon :  0.19609085464969897\n",
            "Step :  600 Loss SD :  1.3785799556427347 Loss G :  1.375566457466443 Epsilon :  0.2122508546497018\n",
            "Step :  700 Loss SD :  1.3670074261965661 Loss G :  1.34615356900938 Epsilon :  0.22841085464970465\n",
            "Step :  800 Loss SD :  1.421447884693504 Loss G :  1.4111774024989487 Epsilon :  0.24422228910496158\n",
            "Step :  900 Loss SD :  1.398183810847983 Loss G :  1.3851689413344026 Epsilon :  0.2591027130895663\n",
            "Step :  1000 Loss SD :  1.4331551238806666 Loss G :  1.419902440657995 Epsilon :  0.27318378194081905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF27hICC5gF1"
      },
      "source": [
        "## Generate New Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxAEg2guxBQT",
        "outputId": "636564f7-5a04-478e-cf46-0b7aae93846e"
      },
      "source": [
        "import pickle\n",
        "\n",
        "syn_data, label = model.generate(1000, class_ratios)\n",
        "syn_data = np.array(syn_data)\n",
        "label = np.array(label)\n",
        "# print(syn_data.shape)\n",
        "# print(label.shape)\n",
        "syn_data = np.squeeze(syn_data)\n",
        "label = np.squeeze(label)\n",
        "print(np.where(label==1))\n",
        "\n",
        "output = open(\"/content/drive/MyDrive/Privacy/ACPATE_results/X_ACPATE_NEW_low_epoch.csv\", 'wb')\n",
        "pickle.dump(syn_data, output)\n",
        "output = open(\"/content/drive/MyDrive/Privacy/ACPATE_results/Y_ACPATE_NEW_low_epoch.csv\", 'wb')\n",
        "pickle.dump(label, output)\n",
        "output.close()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 1000\n",
            "1000 1000\n",
            "(1000, 1, 4)\n",
            "(1000, 1, 1)\n",
            "SHAPE AFTER\n",
            "(1000, 4)\n",
            "(1000,)\n",
            "(array([  2,   5,   6,  11,  12,  13,  14,  15,  17,  19,  21,  22,  24,\n",
            "        25,  26,  27,  28,  29,  32,  33,  35,  39,  42,  43,  45,  47,\n",
            "        49,  56,  57,  58,  60,  61,  62,  63,  65,  66,  67,  68,  72,\n",
            "        73,  74,  77,  78,  79,  81,  82,  90,  91,  92,  94,  97,  98,\n",
            "        99, 100, 103, 104, 105, 106, 107, 110, 112, 115, 116, 117, 119,\n",
            "       123, 125, 127, 129, 130, 132, 133, 137, 139, 140, 141, 143, 145,\n",
            "       147, 152, 154, 160, 161, 162, 167, 169, 170, 173, 174, 175, 177,\n",
            "       179, 180, 181, 182, 186, 187, 189, 190, 191, 194, 195, 196, 197,\n",
            "       201, 202, 203, 205, 206, 207, 208, 213, 217, 219, 220, 221, 222,\n",
            "       224, 228, 229, 233, 235, 236, 237, 238, 243, 244, 246, 254, 256,\n",
            "       263, 265, 271, 275, 276, 278, 280, 281, 282, 284, 286, 287, 288,\n",
            "       293, 296, 299, 301, 304, 307, 308, 309, 313, 314, 316, 317, 318,\n",
            "       321, 322, 328, 330, 332, 333, 334, 340, 343, 344, 347, 348, 351,\n",
            "       353, 355, 360, 362, 363, 366, 367, 369, 372, 373, 378, 380, 381,\n",
            "       382, 384, 385, 387, 389, 391, 393, 395, 398, 400, 402, 404, 406,\n",
            "       409, 413, 414, 420, 421, 422, 425, 426, 428, 429, 430, 432, 434,\n",
            "       435, 437, 441, 442, 446, 449, 451, 452, 453, 454, 455, 458, 462,\n",
            "       464, 465, 467, 468, 469, 471, 472, 473, 474, 477, 478, 485, 488,\n",
            "       490, 498, 499, 501, 504, 505, 508, 510, 511, 513, 518, 520, 521,\n",
            "       529, 530, 531, 533, 537, 538, 540, 542, 545, 547, 550, 551, 552,\n",
            "       554, 558, 559, 560, 561, 562, 563, 565, 567, 570, 571, 572, 574,\n",
            "       578, 581, 582, 583, 586, 587, 590, 591, 592, 593, 598, 600, 601,\n",
            "       602, 604, 605, 611, 612, 620, 624, 625, 626, 630, 632, 635, 638,\n",
            "       639, 640, 641, 642, 643, 644, 646, 649, 652, 655, 656, 659, 662,\n",
            "       664, 665, 669, 671, 673, 677, 684, 685, 687, 688, 689, 691, 692,\n",
            "       693, 695, 696, 700, 701, 702, 703, 705, 706, 709, 714, 717, 718,\n",
            "       719, 721, 723, 724, 728, 729, 732, 733, 737, 739, 740, 744, 749,\n",
            "       751, 752, 754, 757, 759, 765, 766, 767, 769, 771, 775, 776, 778,\n",
            "       781, 784, 786, 787, 790, 792, 793, 796, 797, 800, 803, 805, 806,\n",
            "       807, 808, 809, 813, 820, 822, 829, 830, 833, 836, 841, 843, 845,\n",
            "       848, 849, 853, 855, 856, 857, 858, 859, 862, 863, 867, 870, 872,\n",
            "       876, 877, 882, 883, 884, 886, 887, 889, 891, 892, 895, 898, 901,\n",
            "       902, 904, 906, 909, 910, 912, 913, 918, 921, 924, 928, 932, 935,\n",
            "       937, 938, 940, 942, 951, 953, 958, 959, 961, 962, 966, 971, 973,\n",
            "       974, 976, 977, 978, 980, 981, 982, 985, 991, 993, 994, 995, 997,\n",
            "       999]),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMtOOqfv5zFe"
      },
      "source": [
        "## Load Generated Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8-8iW4HaW9H"
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Privacy/ACPATE_results/X_ACPATE_NEW_low_epoch.csv', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "# print(data)\n",
        "\n",
        "with open('/content/drive/MyDrive/Privacy/ACPATE_results/Y_ACPATE_NEW_low_epoch.csv', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "# print(data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}